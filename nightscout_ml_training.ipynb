{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgHYS2yzxb+hWR9US7zSQn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erobinson/nightscout-ml/blob/main/nightscout_ml_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to nightscout-ml\n"
      ],
      "metadata": {
        "id": "4-Eqn-XgGKY-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bAO8VR-AE2yz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75ad4122-4d6e-4f6a-e87b-c74bf4ab488e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "👍 Found data file at location: /content/drive/My Drive/nightscout-ml/mt/data.xlsx\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "data_folder = '/content/drive/My Drive/nightscout-ml/'\n",
        "data_xlsx_file = data_folder+'/data.xlsx'\n",
        "path = Path(data_xlsx_file)\n",
        "if not path.is_file():\n",
        "  raise Exception(\"Data file not found in location: \"+data_xlsx_file)\n",
        "else:\n",
        "  print(\"👍 Found data file at location: \"+data_xlsx_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "date_format_str = '%m/%d/%y %I:%M%p'\n",
        "\n",
        "def time_to_str(date):\n",
        "    date_str = datetime.strftime(date, date_format_str)\n",
        "    \n",
        "    # drop leading 0 on month\n",
        "    date_str = date_str[1:] if date_str.startswith('0') else date_str\n",
        "    \n",
        "    date_str = date_str.replace('/0', '/')\n",
        "    return date_str\n",
        "\n",
        "def str_to_time(date_str):\n",
        "    return datetime.strptime(date_str, date_format_str)"
      ],
      "metadata": {
        "id": "z5Et-tfKKNCZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from datetime import datetime\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import PReLU\n",
        "import time\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "pd.set_option('display.width', 3000)\n",
        "now = datetime.now()\n",
        "date_str = \"{}-{}-{}_{}-{}\".format(now.year, now.month, now.day, now.hour, now.minute)\n",
        "time_ranges = ['0to60minAgo', '60to120minAgo', '120to180minAgo', '180to240minAgo']\n",
        "log_folder = 'logs'\n",
        "\n",
        "hour_breakdowns = [\"hour0_2\",\"hour3_5\",\"hour6_8\",\"hour9_11\",\"hour12_14\",\"hour15_17\",\"hour18_20\",\"hour21_23\"]\n",
        "accellerating = [\"accelerating_up\",\"deccelerating_up\",\"accelerating_down\",\"deccelerating_down\",\"stable\"]\n",
        "recent_steps = [\"recentSteps5Minutes\",\"recentSteps10Minutes\",\"recentSteps15Minutes\",\"recentSteps30Minutes\",\"recentSteps60Minutes\"]\n",
        "sleep_seated = [\"sleep\",\"sedentary\"]\n",
        "tdd = [\"tdd7Days\",\"tddDaily\",\"tdd24Hrs\"]\n",
        "tddPerHour = [\"tdd7DaysPerHour\",\"tdd2DaysPerHour\",\"tddDailyPerHour\",\"tdd24HrsPerHour\"]\n",
        "base_cols = [\"hourOfDay\",\"weekend\",\n",
        "                \"bg\",\"targetBg\",\"iob\",\"cob\",\"lastCarbAgeMin\",\"futureCarbs\",\n",
        "                \"delta\",\"shortAvgDelta\",\"longAvgDelta\"]\n",
        "cob_delta = ['cobDelta']\n",
        "gi_tags = ['giFast', 'giMedium', 'giSlow']\n",
        "prior_bgs_to_add = 36\n",
        "\n",
        "col_map = {\n",
        "    'best_cols': base_cols+tddPerHour+recent_steps,\n",
        "    # 'best_cols_cob_delta': base_cols+tddPerHour+cob_delta+recent_steps+cob_delta,\n",
        "    # 'best_cols_plus_glycemic_index': base_cols+tddPerHour+recent_steps+gi_tags,\n",
        "    # 'best_cols_plus_fast_med_slow': base_cols+tddPerHour+recent_steps+['fastCarbs','mediumCarbs','slowCarbs']\n",
        "    # 'best_cols_plus_prior_bgs_and_smbs': base_cols+tddPerHour\n",
        "    # 'best_cols_w_accell': base_cols+tddPerHour+cob_delta+recent_steps+accellerating,\n",
        "    \n",
        "    ### meal tags didn't improve model accuracy sadly :(\n",
        "    # 'best_cols_plus_tags': base_cols+tddPerHour+cob_delta+recent_steps\n",
        "\n",
        "    #'base': base_cols, # comment this line\n",
        "    # 'base_cobDelta': base_cols+cob_delta, # helps a little\n",
        "    # 'base_tddPerhour': base_cols+tddPerHour, # helpful\n",
        "    # 'base_recentSteps': base_cols+recent_steps, # helpful\n",
        "    # 'base_accellerating': base_cols+accellerating, # helpful, but could probably refine\n",
        "    \n",
        "    # 'base_tdd': base_cols+tdd, # helps, but tddPerHour values are more effective\n",
        "    # 'base_tdd_tddPerHour': base_cols+tdd+tddPerHour, # not more helpful than tddPerHour\n",
        "    # 'base_recentSteps_sleepSeated': base_cols+recent_steps+sleep_seated, # no major diff, steps is fine\n",
        "    # 'base_sleepSeated': base_cols+sleep_seated, # no major diff thus drop\n",
        "    # 'base_hour_breakdowns': base_cols+hour_breakdowns # not helpful thus drop\n",
        "}\n",
        "\n",
        "# Define model hyper parameters such as input values and nural network dimensions\n",
        "HP_COLS = hp.HParam('cols', hp.Discrete(list(col_map.keys())))\n",
        "\n",
        "HP_NUM_NODES_L1 = hp.HParam('num_nodes_l1', hp.Discrete([5])) \n",
        "HP_NUM_NODES_L2 = hp.HParam('num_nodes_l2', hp.Discrete([0]))\n",
        "HP_NUM_NODES_L3 = hp.HParam('num_nodes_l3', hp.Discrete([0]))\n",
        "HP_NUM_NODES_L4 = hp.HParam('num_nodes_l4', hp.Discrete([0]))\n",
        "HP_NUM_EPOCHS = hp.HParam('num_epochs', hp.Discrete([2]))\n",
        "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([.1]))\n",
        "HP_LAST_ACTIVATION = hp.HParam('last_activation', hp.Discrete(['prelu']))\n",
        "METRIC_LOSS = 'loss'\n",
        "\n",
        "def build_model(run_dir, hparams, train_features, train_labels, test_features, test_labels):\n",
        "    loss = 1\n",
        "    model = ''\n",
        "    with tf.summary.create_file_writer(run_dir).as_default():\n",
        "        hp.hparams(hparams)  # record the values used in this trial\n",
        "        model, loss = train_test_model(hparams, train_features, train_labels, test_features, test_labels)\n",
        "        tf.summary.scalar(METRIC_LOSS, loss, step=1)\n",
        "    return model, loss\n",
        "\n",
        "def train_test_model(hparams, train_features, train_labels, test_features, test_labels):\n",
        "    normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "    normalizer.adapt(np.array(train_features))\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Input(shape=(train_features.shape[1],)))\n",
        "    model.add(normalizer)\n",
        "    if hparams[HP_NUM_NODES_L1] > 0:\n",
        "        model.add(layers.Dense(units=hparams[HP_NUM_NODES_L1], activation=\"relu\"))\n",
        "    if hparams[HP_NUM_NODES_L2] > 0:\n",
        "        model.add(layers.Dense(units=hparams[HP_NUM_NODES_L2], activation=\"relu\"))\n",
        "    if hparams[HP_NUM_NODES_L3] > 0 and hparams[HP_NUM_NODES_L2] > 0:\n",
        "        model.add(layers.Dense(units=hparams[HP_NUM_NODES_L3], activation=\"relu\"))\n",
        "    if hparams[HP_NUM_NODES_L4] > 0 and hparams[HP_NUM_NODES_L3] > 0 and hparams[HP_NUM_NODES_L2] > 0:\n",
        "        model.add(layers.Dense(units=hparams[HP_NUM_NODES_L4], activation=\"relu\"))\n",
        "\n",
        "    if hparams[HP_LAST_ACTIVATION] == 'prelu':\n",
        "        prelu = PReLU()\n",
        "        model.add(layers.Dense(units=1, activation=prelu))\n",
        "    else:\n",
        "        model.add(layers.Dense(units=1, activation=hparams[HP_LAST_ACTIVATION]))\n",
        "\n",
        "    model.compile(\n",
        "        # optimizer=tf.optimizers.Adam(learning_rate=hparams[HP_LEARNING_RATE]),\n",
        "        optimizer='adam',\n",
        "        loss='mean_absolute_error')\n",
        "\n",
        "    model.fit(\n",
        "        train_features,\n",
        "        train_labels,\n",
        "        epochs=hparams[HP_NUM_EPOCHS],\n",
        "        # Suppress logging.\n",
        "        verbose=1,\n",
        "        # Calculate validation results on 20% of the training data.\n",
        "        validation_split = 0.2)\n",
        "    \n",
        "    loss = model.evaluate(test_features, test_labels)\n",
        "    print(train_features.head(10))\n",
        "    print(train_labels.head(10))\n",
        "    print(test_features.head(10))\n",
        "    print(test_labels.head(10))\n",
        "    print(f\"Trained model with {len(model.layers)} layers and loss of {loss}\")\n",
        "    return model, loss\n",
        "\n",
        "def add_glycemic_index_cols(df):\n",
        "    # load vocabulary map from csv\n",
        "    df_gi = pd.read_csv('glycemic_index.csv')\n",
        "    for index, row in df_gi.iterrows():\n",
        "        tag = row['tag']\n",
        "        gi = row['index']\n",
        "        \n",
        "        # if 0-60 min contains fast acting carb word, then set fast GI to 1\n",
        "        if gi == 'fast':\n",
        "            df['giFast'] = df['tags0to60minAgo'].str.contains(tag)\n",
        "            df['giFast'] = df['giFast'].astype(int)\n",
        "        \n",
        "        # if 0-60 or 60-120 contains medium acting carb word, then set medium GI col to 1\n",
        "        if gi == 'medium':\n",
        "            df['giMedium'] = df['tags0to60minAgo'].str.contains(tag)\n",
        "            df['giMedium'] = df['tags60to120minAgo'].str.contains(tag)\n",
        "            df['giMedium'] = df['giMedium'].astype(int)\n",
        "        \n",
        "        # if tags contain slow acting word, set slow GI column to 1\n",
        "        if gi == 'slow':\n",
        "            df['giSlow'] = df['tags60to120minAgo'].str.contains(tag)\n",
        "            df['giSlow'] = df['tags120to180minAgo'].str.contains(tag)\n",
        "            df['giSlow'] = df['tags180to240minAgo'].str.contains(tag)\n",
        "            df['giSlow'] = df['giSlow'].astype(int)\n",
        "        \n",
        "    return df\n",
        "\n",
        "def convert_tags_to_cols(df):\n",
        "    vocabulary = get_vocabulary_from_tags(df)\n",
        "    \n",
        "    # fill in NaN values with empty strings to support setting tags\n",
        "    for range in time_ranges:\n",
        "        df['tags'+range] = df['tags'+range].fillna('')\n",
        "\n",
        "    for word in vocabulary:\n",
        "        for range in time_ranges:\n",
        "            col_name = word+'_'+range\n",
        "            df[col_name] = df['tags'+range].str.contains(word)\n",
        "            df[col_name] = df[col_name].astype(int)\n",
        "    return df, vocabulary\n",
        "\n",
        "def get_vocabulary_from_tags(df):\n",
        "    # get list of unique words\n",
        "    words = list(df['tags0to60minAgo'].str.split(' ', expand=True).stack().unique())\n",
        "    stop_words = ['for', 'a', 'an', 'and', 'of', 'with', '']\n",
        "    \n",
        "    # vocaublary = [word.lower() for word in vocaublary]\n",
        "    vocabulary = []\n",
        "    for word in words:\n",
        "        word = word.lower()\n",
        "        word = word[:-2] if word.endswith(\"es\") else word\n",
        "        word = word[:-1] if word.endswith(\"s\") else word\n",
        "        if word not in stop_words and word not in vocabulary:\n",
        "            vocabulary.append(word)\n",
        "    # strip commas, strip stop words (and, a) - android strips a/an/and\n",
        "    # stem words if need be\n",
        "    return vocabulary\n",
        "\n",
        "def get_basic_model_desc(model):\n",
        "    model_info = \"\\n\\n------------\\n\"\n",
        "    model_info += f\"Model {date_str}\\n\"\n",
        "    model_info += f\"{len(model.layers)} Layers:\\n\"\n",
        "    for i in range (len(model.layers)):\n",
        "        layer = model.layers[i]\n",
        "        layer_info = f\"    - Layer {i}  - {layer.name}\"\n",
        "        layer_info += f\" ({layer.units})\" if 'dense' in layer.name else \"\"\n",
        "        layer_info += f\" ({layer.rate})\" if 'dropout' in layer.name else \"\"\n",
        "        model_info += layer_info + \"\\n\"\n",
        "    return model_info\n",
        "\n",
        "def save_model_info_v2(model, cols, best_loss, num_epochs, data_row_count, training_time, best_last_activation, best_learning_rate, vocabulary):\n",
        "    model_info = get_basic_model_desc(model)\n",
        "\n",
        "    model_info += f\"Model Loss & Accuracy: {str(round(best_loss, 5))}\\n\"\n",
        "    model_info += f\"Number of Epochs: {num_epochs} \\n\"\n",
        "    model_info += f\"Columns ({len(cols)}): {cols} - {col_map[cols]} \\n\"\n",
        "    model_info += f\"Vocabulary ({len(vocabulary)}) {vocabulary}\\n\"\n",
        "    model_info += f\"Training Data Size: {data_row_count} \\n\"\n",
        "    model_info += f\"Learning Rate: {best_learning_rate} \\n\"\n",
        "    model_info += f\"Activation: {best_last_activation} \\n\" if best_last_activation is not None else \"Activation: None\\n\"\n",
        "    # model_info += basic_predictions(model) + \"\\n\"\n",
        "    model_info += f\"Took {time.strftime('%H:%M:%S', time.gmtime(training_time))} to train\\n\"\n",
        "    model_info += \"NOTES: \\n\"\n",
        "    open(data_folder+'/models/tf_model_results.txt', \"a\").write(model_info)\n",
        "\n",
        "\n",
        "\n",
        "def basic_predictions(model):\n",
        "    if len(current_cols) != 38:\n",
        "        return f\"ERROR: incorrect number of columns ({len(current_cols)})\"\n",
        "\n",
        "    low = basic_predict(model,50,0.0,0.0,0)\n",
        "    low_w_iob = basic_predict(model,50,1.0,0.0,0)\n",
        "    normal_w_iob = basic_predict(model,100,1.0,0.0,0)\n",
        "    normal_wo_iob = basic_predict(model,100,0.0,0.0,0)\n",
        "    high_bg = basic_predict(model,200,0.0,0.0,0)\n",
        "    high_cob = basic_predict(model,100,1.0,30.0,0)\n",
        "    high_both = basic_predict(model,200,1.0,30.0,0)\n",
        "    line =  f\"    low: {low}    low_w_iob: {low_w_iob}    normal_w_iob: {normal_w_iob}    normal_wo_iob: {normal_wo_iob}\\n\"\n",
        "    line += f\"    high_bg: {high_bg}    high_cob: {high_cob}    high_both: {high_both}    high_both_and_rising {basic_predict(model, 200, 1, 60, 10)}\\n\"\n",
        "    line += f\"    low_rising  : {basic_predict(model, 70, 0, 20, 10)}    normal_rising  : {basic_predict(model, 100, 0, 20, 10)}    high_rising  : {basic_predict(model, 180, 0, 20, 10)}\\n\"\n",
        "    line += f\"    low_dropping: {basic_predict(model, 70, 2, 0, -7)}    normal_dropping: {basic_predict(model, 100, 2, 0, -7)}    high_dropping: {basic_predict(model, 180, 2, 0, -7)}\\n\"\n",
        "    return line\n",
        "    \n",
        "def basic_predict(model, bg, iob, cob, delta):\n",
        "    last_cob_min = 0 if cob == 0 else 5\n",
        "    accelerating_up = 1 if delta > 3 else 0\n",
        "    deccelerating_down = 1 if delta < -3 else 0\n",
        "    stable= 1 if delta > -3 and delta < 3 else 0\n",
        "    prediction = model.predict([[11,1,0,0, 0,0,0, 0,0,0, \n",
        "                    bg,100,iob,cob,last_cob_min,0,delta,delta,delta,\n",
        "                    accelerating_up, 0, deccelerating_down, 0, stable, \n",
        "                    33,1, 33,1, 33,1,\n",
        "                    0,0,0, 0,0,\n",
        "                    0,1]])\n",
        "                    # \"hourOfDay\",\"hour0_2\",\"hour3_5\",\"hour6_8\", \"hour9_11\",\"hour12_14\",\"hour15_17\", \"hour18_20\",\"hour21_23\",\"weekend\",\n",
        "                    # \"bg\",\"targetBg\",\"iob\",\"cob\",\"lastCarbAgeMin\",\"futureCarbs\",\"delta\",\"shortAvgDelta\",\"longAvgDelta\",\n",
        "                    # \"accelerating_up\",\"deccelerating_up\",\"accelerating_down\",\"deccelerating_down\",\"stable\",\n",
        "                    # \"tdd7Days\",\"tddDaily\",\"tddPerHour\",\"tdd24Hrs\",\n",
        "                    # \"tdd7Days\",\"tdd7DaysPerHour\", \"tddDaily\",\"tddDailyPerHour\", \"tdd24Hrs\",\"tdd24HrsPerHour\",\n",
        "                    # \"recentSteps5Minutes\",\"recentSteps10Minutes\",\"recentSteps15Minutes\", \"recentSteps30Minutes\",\"recentSteps60Minutes\",\n",
        "                    # \"sleep\",\"sedintary\",\n",
        "                    \n",
        "    return str(round(prediction[0][0], 3))\n",
        "\n",
        "def clear_log_folder():\n",
        "    if os.path.exists(log_folder):\n",
        "        shutil.rmtree(log_folder)\n",
        "    os.mkdir(log_folder)\n",
        "    if not os.path.exists(data_folder+'/models'):\n",
        "        os.mkdir(data_folder+'/models')\n",
        "\n",
        "\n",
        "\n",
        "def save_model(model):\n",
        "    # https://www.tensorflow.org/tutorials/keras/save_and_load#savedmodel_format\n",
        "    model.save(data_folder+'/models/backup/tf_model_'+date_str)\n",
        "\n",
        "    # https://medium.com/analytics-vidhya/running-ml-models-in-android-using-tensorflow-lite-e549209287f0\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model=model)\n",
        "    lite_model = converter.convert()\n",
        "    open(data_folder+'/models/backup/tf_model_'+date_str+'.tflite', \"wb\").write(lite_model)\n",
        "    open(data_folder+'/models/model.tflite', \"wb\").write(lite_model)\n",
        "\n",
        "\n",
        "def compare_two_models(model_1_date, model_2_date, row_dates):\n",
        "    m1 = tf.keras.models.load_model(f'{data_folder}/models/backup/tf_model_{model_1_date}')\n",
        "    m2 = tf.keras.models.load_model(f'{data_folder}/models/backup/tf_model_{model_2_date}')\n",
        "\n",
        "    df = pd.read_excel(data_xlsx_file,'training_data')\n",
        "    \n",
        "    # row_1 = row.copy()\n",
        "    # row_1.pop('tdd7DaysPerHour')\n",
        "    # row_1.pop('tdd24HrsPerHour')\n",
        "\n",
        "    eval_features = df.sample(frac=0.3, random_state=0)\n",
        "    eval_features = eval_features[current_cols]\n",
        "    eval_features_1 = eval_features.copy()\n",
        "    # eval_features_1.pop('tdd7DaysPerHour')\n",
        "    # eval_features_1.pop('tdd24HrsPerHour')\n",
        "    eval_features_1.pop('smbToGive')\n",
        "    eval_labels = eval_features.pop('smbToGive')\n",
        "\n",
        "    m1_eval = str(round(m1.evaluate(eval_features_1, eval_labels)[0], 6))\n",
        "    m2_eval = str(round(m2.evaluate(eval_features, eval_labels)[0], 6))\n",
        "    eval_diff = str(round((float(m2_eval) - float(m1_eval)), 5))\n",
        "\n",
        "    # m1_predict = str(round(m1.predict([row_1])[0][0],3))\n",
        "    # m2_predict = str(round(m2.predict([row])[0][0],3))\n",
        "\n",
        "    model_info = \" ---- Model Comparison ----\\n\"\n",
        "    model_info += f\" ---- MODELS: model 1: {model_1_date} - model 2: {model_2_date}\\n\"\n",
        "    model_info += f\" ---- EVAL LOSS:   model 1: {m1_eval} - model 2: {m2_eval} => \"\n",
        "    model_info += f\"{(eval_diff)} getting better \\n\\n\" if float(eval_diff) < 0 else f\"{(eval_diff)} !!! BAD !!! \\n\\n\"\n",
        "    for row_date in row_dates:\n",
        "        row = df.loc[df['dateStr'] == row_date]\n",
        "        row = row[current_cols]\n",
        "        smb_to_give = row['smbToGive'].values[0]\n",
        "        row.pop('smbToGive')\n",
        "        m1_predict = str(round(m1.predict([row])[0][0],3))\n",
        "        m2_predict = str(round(m2.predict([row])[0][0],3))\n",
        "        model_info += f\" ------ PREDICTIONS: {row_date} \"\n",
        "        model_info += f\"({smb_to_give}u, {row['bg'].values[0]}mg/dL, delta: {row['delta'].values[0]}, shortAvgDelta: {row['shortAvgDelta'].values[0]})\"\n",
        "        model_info += f\" - model 1: {m1_predict} - model 2: {m2_predict}\\n\"\n",
        "    \n",
        "    open(data_folder+'/models/tf_model_results.txt', \"a\").write(model_info)\n",
        "\n",
        "\n",
        "\n",
        "def build_tf_regression():\n",
        "    start = time.time()\n",
        "    df = pd.read_excel(data_xlsx_file,'training_data')\n",
        "    # df = df[100:9566]\n",
        "    clear_log_folder()\n",
        "\n",
        "    with tf.summary.create_file_writer(f'{log_folder}/hparam_tuning').as_default():\n",
        "        hp.hparams_config(\n",
        "            hparams=[HP_COLS, HP_NUM_NODES_L1, HP_NUM_NODES_L2, HP_NUM_NODES_L3, HP_NUM_NODES_L4, \n",
        "            HP_NUM_EPOCHS, HP_LEARNING_RATE, HP_LAST_ACTIVATION],\n",
        "            metrics=[hp.Metric(METRIC_LOSS, display_name='loss')],\n",
        "        )\n",
        "\n",
        "    if 'best_cols_plus_glycemic_index' in col_map.keys() or 'best_cols_plus_tags' in col_map.keys():\n",
        "        df, vocabulary = convert_tags_to_cols(df)\n",
        "        if 'best_cols_plus_glycemic_index' in col_map.keys():\n",
        "            df = add_glycemic_index_cols(df)\n",
        "    else:\n",
        "        vocabulary = ['not using tags']\n",
        "    train_dataset = df.sample(frac=0.8, random_state=0)\n",
        "    test_dataset = df.drop(train_dataset.index)\n",
        "\n",
        "    train_features = train_dataset.copy()\n",
        "    test_features = test_dataset.copy()\n",
        "\n",
        "    train_labels = train_features.pop('smbToGive')\n",
        "    test_labels = test_features.pop('smbToGive')\n",
        "\n",
        "    best_loss = 1\n",
        "    best_model = 1\n",
        "    best_epochs = 0\n",
        "    best_last_activation = ''\n",
        "    best_learning_rate = .1\n",
        "    best_cols = ''\n",
        "\n",
        "    \n",
        "    \n",
        "    session_num = 0\n",
        "\n",
        "    for cols_name in HP_COLS.domain.values:\n",
        "        columns = col_map[cols_name]\n",
        "        if 'plus_tags' in cols_name:\n",
        "            for time_range in time_ranges:\n",
        "                for word in vocabulary:\n",
        "                    columns.append(f\"{word}_{time_range}\")\n",
        "        if 'prior_bgs_and_smbs' in cols_name:\n",
        "            for offset in range(0, prior_bgs_to_add):\n",
        "                offset_in_5_min = (offset+1) * 5\n",
        "                columns.append(f\"{offset_in_5_min}_min_prior_bg\")\n",
        "                columns.append(f\"{offset_in_5_min}_min_prior_smb\")\n",
        "\n",
        "        iteration_train_features = train_features[columns]\n",
        "        iteration_test_features = test_features[columns]\n",
        "        for num_nodes_l1 in HP_NUM_NODES_L1.domain.values:\n",
        "            for num_nodes_l2 in HP_NUM_NODES_L2.domain.values:\n",
        "                for num_nodes_l3 in HP_NUM_NODES_L3.domain.values:\n",
        "                    for num_nodes_l4 in HP_NUM_NODES_L4.domain.values:\n",
        "                        for num_epochs in HP_NUM_EPOCHS.domain.values:\n",
        "                            for learn_rate in HP_LEARNING_RATE.domain.values:\n",
        "                                for last_activation in HP_LAST_ACTIVATION.domain.values:\n",
        "                                    hparams = {\n",
        "                                        HP_COLS: cols_name,\n",
        "                                        HP_NUM_NODES_L1: num_nodes_l1,\n",
        "                                        HP_NUM_NODES_L2: num_nodes_l2,\n",
        "                                        HP_NUM_NODES_L3: num_nodes_l3,\n",
        "                                        HP_NUM_NODES_L4: num_nodes_l4,\n",
        "                                        HP_NUM_EPOCHS: num_epochs,\n",
        "                                        HP_LEARNING_RATE: learn_rate,\n",
        "                                        HP_LAST_ACTIVATION: last_activation\n",
        "                                    }\n",
        "                                    run_name = f\"run-{cols_name}-{session_num}\"\n",
        "                                    print(f\"--- Starting trail {run_name}\")\n",
        "                                    print({h.name: hparams[h] for h in hparams})\n",
        "                                    model, loss = build_model('logs/hparam_tuning/' + run_name, hparams, iteration_train_features, train_labels, iteration_test_features, test_labels)\n",
        "                                    session_num += 1\n",
        "                                    if loss < best_loss:\n",
        "                                        best_loss = loss\n",
        "                                        best_model = model\n",
        "                                        best_epochs = num_epochs\n",
        "                                        best_learning_rate = learn_rate\n",
        "                                        best_last_activation = last_activation\n",
        "                                        best_cols = cols_name\n",
        "    \n",
        "    training_time = time.time() - start\n",
        "    \n",
        "    save_model_info_v2(best_model, best_cols, best_loss, best_epochs, len(df), training_time, best_last_activation, best_learning_rate, vocabulary)\n",
        "\n",
        "    save_model(best_model)\n",
        "    \n",
        "    return date_str\n",
        "\n",
        "build_tf_regression()"
      ],
      "metadata": {
        "id": "1kxMrMw0Kdwf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 993
        },
        "outputId": "4b823cb2-838c-476b-87a5-58557307477f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting trail run-best_cols-0\n",
            "{'cols': 'best_cols', 'num_nodes_l1': 5, 'num_nodes_l2': 0, 'num_nodes_l3': 0, 'num_nodes_l4': 0, 'num_epochs': 2, 'learning_rate': 0.1, 'last_activation': 'prelu'}\n",
            "Epoch 1/2\n",
            "71/71 [==============================] - 1s 7ms/step - loss: 0.1056 - val_loss: 0.0757\n",
            "Epoch 2/2\n",
            "71/71 [==============================] - 0s 3ms/step - loss: 0.0824 - val_loss: 0.0724\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0851\n",
            "      hourOfDay  weekend     bg  targetBg       iob  cob  lastCarbAgeMin  futureCarbs  delta  shortAvgDelta  longAvgDelta  tdd7DaysPerHour  tdd2DaysPerHour  tddDailyPerHour  tdd24HrsPerHour  recentSteps5Minutes  recentSteps10Minutes  recentSteps15Minutes  recentSteps30Minutes  recentSteps60Minutes\n",
            "2606          6        0   98.0      90.0  0.035672  0.0            1440          0.0    1.0           0.33          0.69         1.346037         1.353941         1.319368         1.131111                    0                     0                     0                     0                     0\n",
            "2010          0        0  121.0      90.0  0.701838  0.0            1440          0.0    3.0           4.11          4.93         1.324286         1.145490         1.215389         1.227132                    0                    24                    80                   209                   209\n",
            "2804          1        0  140.0      98.5  0.026273  0.0            1440          0.0    6.0           6.89          7.06         1.310305         1.270708         1.222049         1.225083                    0                     0                     0                     0                    27\n",
            "1632         14        1  152.0      90.0  4.605297  0.0            1440          0.0   -5.0          -3.39          1.45         1.260198         1.475615         1.297875         1.255139                  167                   165                     0                   354                   413\n",
            "2372         10        0  128.0      90.0  1.357617  0.0            1440          0.0   -6.0          -5.61         -4.91         1.312606         1.301951         1.388514         1.498201                    0                     0                     0                     0                     0\n",
            "192           8        1  119.0     100.0  0.050086  0.0          184012          0.0    7.0           5.00          1.19         1.185868         1.337885         1.214674         1.110083                    0                    76                    24                   100                   100\n",
            "2922         10        0  241.0      98.5  0.000103  0.0            1440          0.0    0.0           0.89          2.51         1.310305         1.270708         1.222049         1.212486                    0                     0                     0                     0                     0\n",
            "2096          7        0  100.0      90.0  0.094708  0.0            1440          0.0   -3.0          -1.94         -1.16         1.324286         1.145490         1.215389         1.287729                   67                     0                     0                   181                   206\n",
            "2695         16        0   55.0      90.0  1.095800  0.0            1440          0.0    7.0           3.44         -1.81         1.346037         1.353941         1.319368         1.297479                    0                     0                     0                     0                     0\n",
            "2973         14        0  191.0      98.5  4.635094  0.0            1440          0.0   -6.0          -4.17          3.37         1.310305         1.270708         1.222049         1.223368                    0                     0                     0                     0                    53\n",
            "2606    0.00\n",
            "2010    0.00\n",
            "2804    0.00\n",
            "1632    0.00\n",
            "2372    0.00\n",
            "192     0.35\n",
            "2922    0.00\n",
            "2096    0.00\n",
            "2695    0.00\n",
            "2973    0.00\n",
            "Name: smbToGive, dtype: float64\n",
            "    hourOfDay  weekend     bg  targetBg       iob  cob  lastCarbAgeMin  futureCarbs  delta  shortAvgDelta  longAvgDelta  tdd7DaysPerHour  tdd2DaysPerHour  tddDailyPerHour  tdd24HrsPerHour  recentSteps5Minutes  recentSteps10Minutes  recentSteps15Minutes  recentSteps30Minutes  recentSteps60Minutes\n",
            "0           1        0   90.0     100.0  0.261701  0.0          182178          0.0    3.0           2.50          1.67         1.197707         1.323156         1.461097         1.412181                    0                     0                     0                     0                     0\n",
            "3           2        0   98.0     100.0  0.339641  0.0          182196          0.0    0.0           0.61          2.05         1.197707         1.323156         1.461097         1.428986                    0                     0                     0                     0                     0\n",
            "7           4        0  104.0     100.0  0.152863  0.0          182316          0.0    4.0           4.00          4.28         1.197707         1.323156         1.461097         1.432472                    0                     0                     0                     0                     0\n",
            "21          5        0   85.0     100.0  0.135984  0.0          182403          0.0    3.0           2.89          1.30         1.197707         1.323156         1.461097         1.358125                    0                     0                     0                     0                     0\n",
            "24          5        0  102.0     100.0  1.490986  0.0          182429          0.0    6.0           5.00          3.24         1.197707         1.323156         1.461097         1.426986                    0                     0                     0                     0                     0\n",
            "25          5        0  100.0     100.0  1.466089  0.0          182432          0.0    0.0           1.56          2.45         1.197707         1.323156         1.461097         1.426986                    0                     0                     0                     0                     0\n",
            "26          6        0   95.0     100.0  1.414316  0.0          182437          0.0   -4.0          -2.28          1.13         1.197707         1.323156         1.461097         1.428306                    0                     0                     0                     0                     0\n",
            "63         10        0   69.0     100.0  1.735114  0.0          182706          0.0   -4.0          -4.11         -5.09         1.197707         1.323156         1.461097         1.461035                    0                     0                     0                     0                     0\n",
            "67         11        0   99.0     100.0  0.678807  0.0          182766          0.0   12.0          11.39          7.08         1.197707         1.323156         1.461097         1.326750                    0                     0                     0                     6                    88\n",
            "84         13        0  163.0     100.0  5.444465  0.0          182892          0.0    3.0           4.44          7.56         1.197707         1.323156         1.461097         1.417757                    0                     0                     0                     0                   435\n",
            "0     0.0\n",
            "3     0.0\n",
            "7     0.2\n",
            "21    0.0\n",
            "24    0.0\n",
            "25    0.0\n",
            "26    0.0\n",
            "63    0.0\n",
            "67    0.0\n",
            "84    0.0\n",
            "Name: smbToGive, dtype: float64\n",
            "Trained model with 3 layers and loss of 0.08508282899856567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2023-4-3_14-0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing Model\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "file_path = f'{data_folder}/model-2.tflite'\n",
        "if not Path(file_path).is_file():\n",
        "  raise Exception(\"Data file not found in location: \"+data_xlsx_file)\n",
        "\n",
        "# Load TFLite model and allocate tensors.\n",
        "interpreter = tf.lite.Interpreter(model_path=file_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "print(input_details)\n",
        "output_details = interpreter.get_output_details()\n",
        "print(output_details)\n",
        "\n",
        "def run_prediction(input_data):\n",
        "  # https://stackoverflow.com/questions/50443411/how-to-load-a-tflite-model-in-script\n",
        "  interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "  interpreter.invoke()\n",
        "  # The function `get_tensor()` returns a copy of the tensor data.\n",
        "  # Use `tensor()` in order to get a pointer to the tensor.\n",
        "  output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "  return output_data\n",
        "\n",
        "\n",
        "# Test model on random input data.\n",
        "input_shape = input_details[0]['shape']\n",
        "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
        "print(\"prediction with random data: \"+str(run_prediction(input_data)))\n",
        "\n",
        "# input columns: \n",
        "#   'hourOfDay', 'weekend', 'bg', 'targetBg', 'iob', 'cob', 'lastCarbAgeMin', 'futureCarbs', \n",
        "#   'delta', 'shortAvgDelta', 'longAvgDelta', \n",
        "#   'tdd7DaysPerHour', 'tdd2DaysPerHour', 'tddDailyPerHour', 'tdd24HrsPerHour', \n",
        "#   'recentSteps5Minutes', 'recentSteps10Minutes', 'recentSteps15Minutes', 'recentSteps30Minutes', 'recentSteps60Minutes'\n",
        "input_data = [[0, 1, 200, 100, 1, 60, 5, 0,\n",
        "               10, 10, 10,\n",
        "               1.5, 1.5, 1.5, 1.5,\n",
        "               10, 10, 10, 10, 10]]\n",
        "for i in range(len(input_data[0])):\n",
        "  # model requires float 32 numbers\n",
        "  input_data[0][i] = np.float32(input_data[0][i])\n",
        "\n",
        "print(\"prediction with input data: \"+str(run_prediction(input_data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "j7BtzaJNYuvg",
        "outputId": "f8fdb070-6871-4fa4-d41f-103390fb9ba9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'name': 'serving_default_input_1:0', 'index': 0, 'shape': array([ 1, 25], dtype=int32), 'shape_signature': array([-1, 25], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "[{'name': 'StatefulPartitionedCall:0', 'index': 12, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([-1,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "prediction with random data: [[-137.01595]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-20ba4c0906ec>\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prediction with input data: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-20ba4c0906ec>\u001b[0m in \u001b[0;36mrun_prediction\u001b[0;34m(input_data)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;31m# https://stackoverflow.com/questions/50443411/how-to-load-a-tflite-model-in-script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;31m# The function `get_tensor()` returns a copy of the tensor data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/interpreter.py\u001b[0m in \u001b[0;36mset_tensor\u001b[0;34m(self, tensor_index, value)\u001b[0m\n\u001b[1;32m    694\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minterpreter\u001b[0m \u001b[0mcould\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mset\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m     \"\"\"\n\u001b[0;32m--> 696\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSetTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mresize_tensor_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot set tensor: Dimension mismatch. Got 20 but expected 25 for dimension 1 of input 0."
          ]
        }
      ]
    }
  ]
}