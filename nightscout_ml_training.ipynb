{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjQpVqg6EPb8nwRVHu7hE4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erobinson/nightscout-ml/blob/main/nightscout_ml_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to nightscout-ml\n"
      ],
      "metadata": {
        "id": "4-Eqn-XgGKY-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAO8VR-AE2yz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d27e4da-0f7e-4d4a-fe2e-89b93175c86e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "👍 Found data file at location: /content/drive/My Drive/nightscout-ml/data.xlsx\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "data_folder = '/content/drive/My Drive/nightscout-ml'\n",
        "data_xlsx_file = data_folder+'/data.xlsx'\n",
        "path = Path(data_xlsx_file)\n",
        "if not path.is_file():\n",
        "  raise Exception(\"Data file not found in location: \"+data_xlsx_file)\n",
        "else:\n",
        "  print(\"👍 Found data file at location: \"+data_xlsx_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "date_format_str = '%m/%d/%y %I:%M%p'\n",
        "\n",
        "def time_to_str(date):\n",
        "    date_str = datetime.strftime(date, date_format_str)\n",
        "    \n",
        "    # drop leading 0 on month\n",
        "    date_str = date_str[1:] if date_str.startswith('0') else date_str\n",
        "    \n",
        "    date_str = date_str.replace('/0', '/')\n",
        "    return date_str\n",
        "\n",
        "def str_to_time(date_str):\n",
        "    return datetime.strptime(date_str, date_format_str)"
      ],
      "metadata": {
        "id": "z5Et-tfKKNCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from datetime import datetime\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import PReLU\n",
        "import time\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "pd.set_option('display.width', 3000)\n",
        "now = datetime.now()\n",
        "date_str = \"{}-{}-{}_{}-{}\".format(now.year, now.month, now.day, now.hour, now.minute)\n",
        "time_ranges = ['0to60minAgo', '60to120minAgo', '120to180minAgo', '180to240minAgo']\n",
        "log_folder = 'logs'\n",
        "\n",
        "hour_breakdowns = [\"hour0_2\",\"hour3_5\",\"hour6_8\",\"hour9_11\",\"hour12_14\",\"hour15_17\",\"hour18_20\",\"hour21_23\"]\n",
        "accellerating = [\"accelerating_up\",\"deccelerating_up\",\"accelerating_down\",\"deccelerating_down\",\"stable\"]\n",
        "recent_steps = [\"recentSteps5Minutes\",\"recentSteps10Minutes\",\"recentSteps15Minutes\",\"recentSteps30Minutes\",\"recentSteps60Minutes\"]\n",
        "sleep_seated = [\"sleep\",\"sedentary\"]\n",
        "tdd = [\"tdd7Days\",\"tddDaily\",\"tdd24Hrs\"]\n",
        "tddPerHour = [\"tdd7DaysPerHour\",\"tdd2DaysPerHour\",\"tddDailyPerHour\",\"tdd24HrsPerHour\"]\n",
        "base_cols = [\"hourOfDay\",\"weekend\",\n",
        "                \"bg\",\"targetBg\",\"iob\",\"cob\",\"lastCarbAgeMin\",\"futureCarbs\",\n",
        "                \"delta\",\"shortAvgDelta\",\"longAvgDelta\"]\n",
        "cob_delta = ['cobDelta']\n",
        "gi_tags = ['giFast', 'giMedium', 'giSlow']\n",
        "prior_bgs_to_add = 36\n",
        "\n",
        "col_map = {\n",
        "    #'best_cols': base_cols+tddPerHour+recent_steps,\n",
        "    # 'best_cols_cob_delta': base_cols+tddPerHour+cob_delta+recent_steps+cob_delta,\n",
        "    # 'best_cols_plus_glycemic_index': base_cols+tddPerHour+recent_steps+gi_tags,\n",
        "    # 'best_cols_plus_fast_med_slow': base_cols+tddPerHour+recent_steps+['fastCarbs','mediumCarbs','slowCarbs']\n",
        "    # 'best_cols_plus_prior_bgs_and_smbs': base_cols+tddPerHour\n",
        "    # 'best_cols_w_accell': base_cols+tddPerHour+cob_delta+recent_steps+accellerating,\n",
        "    \n",
        "    ### meal tags didn't improve model accuracy sadly :(\n",
        "    # 'best_cols_plus_tags': base_cols+tddPerHour+cob_delta+recent_steps\n",
        "\n",
        "    'base': base_cols,\n",
        "    # 'base_cobDelta': base_cols+cob_delta, # helps a little\n",
        "    # 'base_tddPerhour': base_cols+tddPerHour, # helpful\n",
        "    # 'base_recentSteps': base_cols+recent_steps, # helpful\n",
        "    # 'base_accellerating': base_cols+accellerating, # helpful, but could probably refine\n",
        "    \n",
        "    # 'base_tdd': base_cols+tdd, # helps, but tddPerHour values are more effective\n",
        "    # 'base_tdd_tddPerHour': base_cols+tdd+tddPerHour, # not more helpful than tddPerHour\n",
        "    # 'base_recentSteps_sleepSeated': base_cols+recent_steps+sleep_seated, # no major diff, steps is fine\n",
        "    # 'base_sleepSeated': base_cols+sleep_seated, # no major diff thus drop\n",
        "    # 'base_hour_breakdowns': base_cols+hour_breakdowns # not helpful thus drop\n",
        "}\n",
        "\n",
        "# Define model hyper parameters such as input values and nural network dimensions\n",
        "HP_COLS = hp.HParam('cols', hp.Discrete(list(col_map.keys())))\n",
        "\n",
        "HP_NUM_NODES_L1 = hp.HParam('num_nodes_l1', hp.Discrete([5])) \n",
        "HP_NUM_NODES_L2 = hp.HParam('num_nodes_l2', hp.Discrete([0]))\n",
        "HP_NUM_NODES_L3 = hp.HParam('num_nodes_l3', hp.Discrete([0]))\n",
        "HP_NUM_NODES_L4 = hp.HParam('num_nodes_l4', hp.Discrete([0]))\n",
        "HP_NUM_EPOCHS = hp.HParam('num_epochs', hp.Discrete([2]))\n",
        "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([.1]))\n",
        "HP_LAST_ACTIVATION = hp.HParam('last_activation', hp.Discrete(['prelu']))\n",
        "METRIC_LOSS = 'loss'\n",
        "\n",
        "def build_model(run_dir, hparams, train_features, train_labels, test_features, test_labels):\n",
        "    loss = 1\n",
        "    model = ''\n",
        "    with tf.summary.create_file_writer(run_dir).as_default():\n",
        "        hp.hparams(hparams)  # record the values used in this trial\n",
        "        model, loss = train_test_model(hparams, train_features, train_labels, test_features, test_labels)\n",
        "        tf.summary.scalar(METRIC_LOSS, loss, step=1)\n",
        "    return model, loss\n",
        "\n",
        "def train_test_model(hparams, train_features, train_labels, test_features, test_labels):\n",
        "    normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "    normalizer.adapt(np.array(train_features))\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Input(shape=(train_features.shape[1],)))\n",
        "    model.add(normalizer)\n",
        "    if hparams[HP_NUM_NODES_L1] > 0:\n",
        "        model.add(layers.Dense(units=hparams[HP_NUM_NODES_L1], activation=\"relu\"))\n",
        "    if hparams[HP_NUM_NODES_L2] > 0:\n",
        "        model.add(layers.Dense(units=hparams[HP_NUM_NODES_L2], activation=\"relu\"))\n",
        "    if hparams[HP_NUM_NODES_L3] > 0 and hparams[HP_NUM_NODES_L2] > 0:\n",
        "        model.add(layers.Dense(units=hparams[HP_NUM_NODES_L3], activation=\"relu\"))\n",
        "    if hparams[HP_NUM_NODES_L4] > 0 and hparams[HP_NUM_NODES_L3] > 0 and hparams[HP_NUM_NODES_L2] > 0:\n",
        "        model.add(layers.Dense(units=hparams[HP_NUM_NODES_L4], activation=\"relu\"))\n",
        "\n",
        "    if hparams[HP_LAST_ACTIVATION] == 'prelu':\n",
        "        prelu = PReLU()\n",
        "        model.add(layers.Dense(units=1, activation=prelu))\n",
        "    else:\n",
        "        model.add(layers.Dense(units=1, activation=hparams[HP_LAST_ACTIVATION]))\n",
        "\n",
        "    model.compile(\n",
        "        # optimizer=tf.optimizers.Adam(learning_rate=hparams[HP_LEARNING_RATE]),\n",
        "        optimizer='adam',\n",
        "        loss='mean_absolute_error')\n",
        "\n",
        "    model.fit(\n",
        "        train_features,\n",
        "        train_labels,\n",
        "        epochs=hparams[HP_NUM_EPOCHS],\n",
        "        # Suppress logging.\n",
        "        verbose=1,\n",
        "        # Calculate validation results on 20% of the training data.\n",
        "        validation_split = 0.2)\n",
        "    \n",
        "    loss = model.evaluate(test_features, test_labels)\n",
        "    print(train_features.head(10))\n",
        "    print(train_labels.head(10))\n",
        "    print(test_features.head(10))\n",
        "    print(test_labels.head(10))\n",
        "    print(f\"Trained model with {len(model.layers)} layers and loss of {loss}\")\n",
        "    return model, loss\n",
        "\n",
        "def add_glycemic_index_cols(df):\n",
        "    # load vocabulary map from csv\n",
        "    df_gi = pd.read_csv('glycemic_index.csv')\n",
        "    for index, row in df_gi.iterrows():\n",
        "        tag = row['tag']\n",
        "        gi = row['index']\n",
        "        \n",
        "        # if 0-60 min contains fast acting carb word, then set fast GI to 1\n",
        "        if gi == 'fast':\n",
        "            df['giFast'] = df['tags0to60minAgo'].str.contains(tag)\n",
        "            df['giFast'] = df['giFast'].astype(int)\n",
        "        \n",
        "        # if 0-60 or 60-120 contains medium acting carb word, then set medium GI col to 1\n",
        "        if gi == 'medium':\n",
        "            df['giMedium'] = df['tags0to60minAgo'].str.contains(tag)\n",
        "            df['giMedium'] = df['tags60to120minAgo'].str.contains(tag)\n",
        "            df['giMedium'] = df['giMedium'].astype(int)\n",
        "        \n",
        "        # if tags contain slow acting word, set slow GI column to 1\n",
        "        if gi == 'slow':\n",
        "            df['giSlow'] = df['tags60to120minAgo'].str.contains(tag)\n",
        "            df['giSlow'] = df['tags120to180minAgo'].str.contains(tag)\n",
        "            df['giSlow'] = df['tags180to240minAgo'].str.contains(tag)\n",
        "            df['giSlow'] = df['giSlow'].astype(int)\n",
        "        \n",
        "    return df\n",
        "\n",
        "def convert_tags_to_cols(df):\n",
        "    vocabulary = get_vocabulary_from_tags(df)\n",
        "    \n",
        "    # fill in NaN values with empty strings to support setting tags\n",
        "    for range in time_ranges:\n",
        "        df['tags'+range] = df['tags'+range].fillna('')\n",
        "\n",
        "    for word in vocabulary:\n",
        "        for range in time_ranges:\n",
        "            col_name = word+'_'+range\n",
        "            df[col_name] = df['tags'+range].str.contains(word)\n",
        "            df[col_name] = df[col_name].astype(int)\n",
        "    return df, vocabulary\n",
        "\n",
        "def get_vocabulary_from_tags(df):\n",
        "    # get list of unique words\n",
        "    words = list(df['tags0to60minAgo'].str.split(' ', expand=True).stack().unique())\n",
        "    stop_words = ['for', 'a', 'an', 'and', 'of', 'with', '']\n",
        "    \n",
        "    # vocaublary = [word.lower() for word in vocaublary]\n",
        "    vocabulary = []\n",
        "    for word in words:\n",
        "        word = word.lower()\n",
        "        word = word[:-2] if word.endswith(\"es\") else word\n",
        "        word = word[:-1] if word.endswith(\"s\") else word\n",
        "        if word not in stop_words and word not in vocabulary:\n",
        "            vocabulary.append(word)\n",
        "    # strip commas, strip stop words (and, a) - android strips a/an/and\n",
        "    # stem words if need be\n",
        "    return vocabulary\n",
        "\n",
        "def get_basic_model_desc(model):\n",
        "    model_info = \"\\n\\n------------\\n\"\n",
        "    model_info += f\"Model {date_str}\\n\"\n",
        "    model_info += f\"{len(model.layers)} Layers:\\n\"\n",
        "    for i in range (len(model.layers)):\n",
        "        layer = model.layers[i]\n",
        "        layer_info = f\"    - Layer {i}  - {layer.name}\"\n",
        "        layer_info += f\" ({layer.units})\" if 'dense' in layer.name else \"\"\n",
        "        layer_info += f\" ({layer.rate})\" if 'dropout' in layer.name else \"\"\n",
        "        model_info += layer_info + \"\\n\"\n",
        "    return model_info\n",
        "\n",
        "def save_model_info_v2(model, cols, best_loss, num_epochs, data_row_count, training_time, best_last_activation, best_learning_rate, vocabulary):\n",
        "    model_info = get_basic_model_desc(model)\n",
        "\n",
        "    model_info += f\"Model Loss & Accuracy: {str(round(best_loss, 5))}\\n\"\n",
        "    model_info += f\"Number of Epochs: {num_epochs} \\n\"\n",
        "    model_info += f\"Columns ({len(cols)}): {cols} - {col_map[cols]} \\n\"\n",
        "    model_info += f\"Vocabulary ({len(vocabulary)}) {vocabulary}\\n\"\n",
        "    model_info += f\"Training Data Size: {data_row_count} \\n\"\n",
        "    model_info += f\"Learning Rate: {best_learning_rate} \\n\"\n",
        "    model_info += f\"Activation: {best_last_activation} \\n\" if best_last_activation is not None else \"Activation: None\\n\"\n",
        "    # model_info += basic_predictions(model) + \"\\n\"\n",
        "    model_info += f\"Took {time.strftime('%H:%M:%S', time.gmtime(training_time))} to train\\n\"\n",
        "    model_info += \"NOTES: \\n\"\n",
        "    open(data_folder+'/models/tf_model_results.txt', \"a\").write(model_info)\n",
        "\n",
        "\n",
        "\n",
        "def basic_predictions(model):\n",
        "    if len(current_cols) != 38:\n",
        "        return f\"ERROR: incorrect number of columns ({len(current_cols)})\"\n",
        "\n",
        "    low = basic_predict(model,50,0.0,0.0,0)\n",
        "    low_w_iob = basic_predict(model,50,1.0,0.0,0)\n",
        "    normal_w_iob = basic_predict(model,100,1.0,0.0,0)\n",
        "    normal_wo_iob = basic_predict(model,100,0.0,0.0,0)\n",
        "    high_bg = basic_predict(model,200,0.0,0.0,0)\n",
        "    high_cob = basic_predict(model,100,1.0,30.0,0)\n",
        "    high_both = basic_predict(model,200,1.0,30.0,0)\n",
        "    line =  f\"    low: {low}    low_w_iob: {low_w_iob}    normal_w_iob: {normal_w_iob}    normal_wo_iob: {normal_wo_iob}\\n\"\n",
        "    line += f\"    high_bg: {high_bg}    high_cob: {high_cob}    high_both: {high_both}    high_both_and_rising {basic_predict(model, 200, 1, 60, 10)}\\n\"\n",
        "    line += f\"    low_rising  : {basic_predict(model, 70, 0, 20, 10)}    normal_rising  : {basic_predict(model, 100, 0, 20, 10)}    high_rising  : {basic_predict(model, 180, 0, 20, 10)}\\n\"\n",
        "    line += f\"    low_dropping: {basic_predict(model, 70, 2, 0, -7)}    normal_dropping: {basic_predict(model, 100, 2, 0, -7)}    high_dropping: {basic_predict(model, 180, 2, 0, -7)}\\n\"\n",
        "    return line\n",
        "    \n",
        "def basic_predict(model, bg, iob, cob, delta):\n",
        "    last_cob_min = 0 if cob == 0 else 5\n",
        "    accelerating_up = 1 if delta > 3 else 0\n",
        "    deccelerating_down = 1 if delta < -3 else 0\n",
        "    stable= 1 if delta > -3 and delta < 3 else 0\n",
        "    prediction = model.predict([[11,1,0,0, 0,0,0, 0,0,0, \n",
        "                    bg,100,iob,cob,last_cob_min,0,delta,delta,delta,\n",
        "                    accelerating_up, 0, deccelerating_down, 0, stable, \n",
        "                    33,1, 33,1, 33,1,\n",
        "                    0,0,0, 0,0,\n",
        "                    0,1]])\n",
        "                    # \"hourOfDay\",\"hour0_2\",\"hour3_5\",\"hour6_8\", \"hour9_11\",\"hour12_14\",\"hour15_17\", \"hour18_20\",\"hour21_23\",\"weekend\",\n",
        "                    # \"bg\",\"targetBg\",\"iob\",\"cob\",\"lastCarbAgeMin\",\"futureCarbs\",\"delta\",\"shortAvgDelta\",\"longAvgDelta\",\n",
        "                    # \"accelerating_up\",\"deccelerating_up\",\"accelerating_down\",\"deccelerating_down\",\"stable\",\n",
        "                    # \"tdd7Days\",\"tddDaily\",\"tddPerHour\",\"tdd24Hrs\",\n",
        "                    # \"tdd7Days\",\"tdd7DaysPerHour\", \"tddDaily\",\"tddDailyPerHour\", \"tdd24Hrs\",\"tdd24HrsPerHour\",\n",
        "                    # \"recentSteps5Minutes\",\"recentSteps10Minutes\",\"recentSteps15Minutes\", \"recentSteps30Minutes\",\"recentSteps60Minutes\",\n",
        "                    # \"sleep\",\"sedintary\",\n",
        "                    \n",
        "    return str(round(prediction[0][0], 3))\n",
        "\n",
        "def clear_log_folder():\n",
        "    if os.path.exists(log_folder):\n",
        "        shutil.rmtree(log_folder)\n",
        "    os.mkdir(log_folder)\n",
        "    if not os.path.exists(data_folder+'/models'):\n",
        "        os.mkdir(data_folder+'/models')\n",
        "\n",
        "\n",
        "\n",
        "def save_model(model):\n",
        "    # https://www.tensorflow.org/tutorials/keras/save_and_load#savedmodel_format\n",
        "    model.save(data_folder+'/models/backup/tf_model_'+date_str)\n",
        "\n",
        "    # https://medium.com/analytics-vidhya/running-ml-models-in-android-using-tensorflow-lite-e549209287f0\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model=model)\n",
        "    lite_model = converter.convert()\n",
        "    open(data_folder+'/models/backup/tf_model_'+date_str+'.tflite', \"wb\").write(lite_model)\n",
        "    open(data_folder+'/models/model.tflite', \"wb\").write(lite_model)\n",
        "\n",
        "\n",
        "def compare_two_models(model_1_date, model_2_date, row_dates):\n",
        "    m1 = tf.keras.models.load_model(f'{data_folder}/models/backup/tf_model_{model_1_date}')\n",
        "    m2 = tf.keras.models.load_model(f'{data_folder}/models/backup/tf_model_{model_2_date}')\n",
        "\n",
        "    df = pd.read_excel(data_xlsx_file,'training_data')\n",
        "    \n",
        "    # row_1 = row.copy()\n",
        "    # row_1.pop('tdd7DaysPerHour')\n",
        "    # row_1.pop('tdd24HrsPerHour')\n",
        "\n",
        "    eval_features = df.sample(frac=0.3, random_state=0)\n",
        "    eval_features = eval_features[current_cols]\n",
        "    eval_features_1 = eval_features.copy()\n",
        "    # eval_features_1.pop('tdd7DaysPerHour')\n",
        "    # eval_features_1.pop('tdd24HrsPerHour')\n",
        "    eval_features_1.pop('smbToGive')\n",
        "    eval_labels = eval_features.pop('smbToGive')\n",
        "\n",
        "    m1_eval = str(round(m1.evaluate(eval_features_1, eval_labels)[0], 6))\n",
        "    m2_eval = str(round(m2.evaluate(eval_features, eval_labels)[0], 6))\n",
        "    eval_diff = str(round((float(m2_eval) - float(m1_eval)), 5))\n",
        "\n",
        "    # m1_predict = str(round(m1.predict([row_1])[0][0],3))\n",
        "    # m2_predict = str(round(m2.predict([row])[0][0],3))\n",
        "\n",
        "    model_info = \" ---- Model Comparison ----\\n\"\n",
        "    model_info += f\" ---- MODELS: model 1: {model_1_date} - model 2: {model_2_date}\\n\"\n",
        "    model_info += f\" ---- EVAL LOSS:   model 1: {m1_eval} - model 2: {m2_eval} => \"\n",
        "    model_info += f\"{(eval_diff)} getting better \\n\\n\" if float(eval_diff) < 0 else f\"{(eval_diff)} !!! BAD !!! \\n\\n\"\n",
        "    for row_date in row_dates:\n",
        "        row = df.loc[df['dateStr'] == row_date]\n",
        "        row = row[current_cols]\n",
        "        smb_to_give = row['smbToGive'].values[0]\n",
        "        row.pop('smbToGive')\n",
        "        m1_predict = str(round(m1.predict([row])[0][0],3))\n",
        "        m2_predict = str(round(m2.predict([row])[0][0],3))\n",
        "        model_info += f\" ------ PREDICTIONS: {row_date} \"\n",
        "        model_info += f\"({smb_to_give}u, {row['bg'].values[0]}mg/dL, delta: {row['delta'].values[0]}, shortAvgDelta: {row['shortAvgDelta'].values[0]})\"\n",
        "        model_info += f\" - model 1: {m1_predict} - model 2: {m2_predict}\\n\"\n",
        "    \n",
        "    open(data_folder+'/models/tf_model_results.txt', \"a\").write(model_info)\n",
        "\n",
        "\n",
        "\n",
        "def build_tf_regression():\n",
        "    start = time.time()\n",
        "    df = pd.read_excel(data_xlsx_file,'training_data')\n",
        "    df = df[100:9566]\n",
        "    clear_log_folder()\n",
        "\n",
        "    with tf.summary.create_file_writer(f'{log_folder}/hparam_tuning').as_default():\n",
        "        hp.hparams_config(\n",
        "            hparams=[HP_COLS, HP_NUM_NODES_L1, HP_NUM_NODES_L2, HP_NUM_NODES_L3, HP_NUM_NODES_L4, \n",
        "            HP_NUM_EPOCHS, HP_LEARNING_RATE, HP_LAST_ACTIVATION],\n",
        "            metrics=[hp.Metric(METRIC_LOSS, display_name='loss')],\n",
        "        )\n",
        "\n",
        "    if 'best_cols_plus_glycemic_index' in col_map.keys() or 'best_cols_plus_tags' in col_map.keys():\n",
        "        df, vocabulary = convert_tags_to_cols(df)\n",
        "        if 'best_cols_plus_glycemic_index' in col_map.keys():\n",
        "            df = add_glycemic_index_cols(df)\n",
        "    else:\n",
        "        vocabulary = ['not using tags']\n",
        "    train_dataset = df.sample(frac=0.8, random_state=0)\n",
        "    test_dataset = df.drop(train_dataset.index)\n",
        "\n",
        "    train_features = train_dataset.copy()\n",
        "    test_features = test_dataset.copy()\n",
        "\n",
        "    train_labels = train_features.pop('smbToGive')\n",
        "    test_labels = test_features.pop('smbToGive')\n",
        "\n",
        "    best_loss = 1\n",
        "    best_model = 1\n",
        "    best_epochs = 0\n",
        "    best_last_activation = ''\n",
        "    best_learning_rate = .1\n",
        "    best_cols = ''\n",
        "\n",
        "    \n",
        "    \n",
        "    session_num = 0\n",
        "\n",
        "    for cols_name in HP_COLS.domain.values:\n",
        "        columns = col_map[cols_name]\n",
        "        if 'plus_tags' in cols_name:\n",
        "            for time_range in time_ranges:\n",
        "                for word in vocabulary:\n",
        "                    columns.append(f\"{word}_{time_range}\")\n",
        "        if 'prior_bgs_and_smbs' in cols_name:\n",
        "            for offset in range(0, prior_bgs_to_add):\n",
        "                offset_in_5_min = (offset+1) * 5\n",
        "                columns.append(f\"{offset_in_5_min}_min_prior_bg\")\n",
        "                columns.append(f\"{offset_in_5_min}_min_prior_smb\")\n",
        "\n",
        "        iteration_train_features = train_features[columns]\n",
        "        iteration_test_features = test_features[columns]\n",
        "        for num_nodes_l1 in HP_NUM_NODES_L1.domain.values:\n",
        "            for num_nodes_l2 in HP_NUM_NODES_L2.domain.values:\n",
        "                for num_nodes_l3 in HP_NUM_NODES_L3.domain.values:\n",
        "                    for num_nodes_l4 in HP_NUM_NODES_L4.domain.values:\n",
        "                        for num_epochs in HP_NUM_EPOCHS.domain.values:\n",
        "                            for learn_rate in HP_LEARNING_RATE.domain.values:\n",
        "                                for last_activation in HP_LAST_ACTIVATION.domain.values:\n",
        "                                    hparams = {\n",
        "                                        HP_COLS: cols_name,\n",
        "                                        HP_NUM_NODES_L1: num_nodes_l1,\n",
        "                                        HP_NUM_NODES_L2: num_nodes_l2,\n",
        "                                        HP_NUM_NODES_L3: num_nodes_l3,\n",
        "                                        HP_NUM_NODES_L4: num_nodes_l4,\n",
        "                                        HP_NUM_EPOCHS: num_epochs,\n",
        "                                        HP_LEARNING_RATE: learn_rate,\n",
        "                                        HP_LAST_ACTIVATION: last_activation\n",
        "                                    }\n",
        "                                    run_name = f\"run-{cols_name}-{session_num}\"\n",
        "                                    print(f\"--- Starting trail {run_name}\")\n",
        "                                    print({h.name: hparams[h] for h in hparams})\n",
        "                                    model, loss = build_model('logs/hparam_tuning/' + run_name, hparams, iteration_train_features, train_labels, iteration_test_features, test_labels)\n",
        "                                    session_num += 1\n",
        "                                    if loss < best_loss:\n",
        "                                        best_loss = loss\n",
        "                                        best_model = model\n",
        "                                        best_epochs = num_epochs\n",
        "                                        best_learning_rate = learn_rate\n",
        "                                        best_last_activation = last_activation\n",
        "                                        best_cols = cols_name\n",
        "    \n",
        "    training_time = time.time() - start\n",
        "    \n",
        "    save_model_info_v2(best_model, best_cols, best_loss, best_epochs, len(df), training_time, best_last_activation, best_learning_rate, vocabulary)\n",
        "\n",
        "    save_model(best_model)\n",
        "    \n",
        "    return date_str\n",
        "\n",
        "build_tf_regression()"
      ],
      "metadata": {
        "id": "1kxMrMw0Kdwf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        },
        "outputId": "4dc4493e-964d-4eda-fc58-c79c8fc65ad7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting trail run-base-0\n",
            "{'cols': 'base', 'num_nodes_l1': 5, 'num_nodes_l2': 0, 'num_nodes_l3': 0, 'num_nodes_l4': 0, 'num_epochs': 2, 'learning_rate': 0.1, 'last_activation': 'prelu'}\n",
            "Epoch 1/2\n",
            "190/190 [==============================] - 1s 3ms/step - loss: 0.1816 - val_loss: 0.1452\n",
            "Epoch 2/2\n",
            "190/190 [==============================] - 0s 3ms/step - loss: 0.1352 - val_loss: 0.1289\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.1246\n",
            "      hourOfDay  weekend   bg  targetBg       iob        cob  lastCarbAgeMin  futureCarbs  delta  shortAvgDelta  longAvgDelta\n",
            "2621         16        0   99       100  1.566288   0.000000             155            0      1           1.22          3.52\n",
            "136          15        0   53       100  1.995151   0.000000              20            0     -1          -0.61         -1.92\n",
            "1707          2        0  182       100  2.036584   0.000000             390            0     11          10.11          6.74\n",
            "5156          8        0  124       100  2.233412   0.000000             900            0     -1          -0.11          3.31\n",
            "7947          9        0  179       100  4.266741  11.714167             101            0      6           5.39          3.47\n",
            "1756          6        0  104       100  0.209048   0.000000             635            0      3           2.89          1.22\n",
            "1434          5        0  169       100  1.783507   0.000000             610            0      1           1.11          1.42\n",
            "7775         19        0  126       100  3.640062  15.210000             112            0      3           0.11         -6.57\n",
            "2911          0        1  100       120  2.018264   0.000000             315            0     -7          -8.00        -12.07\n",
            "4240          1        1  132       100  0.660993   0.000000             478            0     -1          -0.72         -0.04\n",
            "2621    0.00\n",
            "136     0.00\n",
            "1707    0.85\n",
            "5156    0.00\n",
            "7947    0.30\n",
            "1756    0.05\n",
            "1434    0.20\n",
            "7775    0.00\n",
            "2911    0.00\n",
            "4240    0.05\n",
            "Name: smbToGive, dtype: float64\n",
            "     hourOfDay  weekend   bg  targetBg       iob        cob  lastCarbAgeMin  futureCarbs  delta  shortAvgDelta  longAvgDelta\n",
            "100         12        0  121       100  2.474367  23.388334               0            0      8           7.67          2.24\n",
            "121         14        0  116       100  5.618461   7.988333              10            0    -14         -14.28        -12.78\n",
            "125         14        0   78       100  4.445606  11.405000               0            0     -8          -8.50        -10.93\n",
            "128         15        0   67       100  3.664160   9.946667               5            0     -2          -3.00         -6.34\n",
            "135         15        0   56       100  2.150161   0.000000              16            0      2           0.78         -1.86\n",
            "143         16        0  133       100  2.044125   0.000000              55            0     14          13.22         11.68\n",
            "160         17        0  105       100  1.518361   0.000000             135            0     -2          -2.67         -3.54\n",
            "162         17        0  100       100  1.423659   0.000000             145            0     -3          -2.61         -2.96\n",
            "163         17        0  100       100  1.428462  30.000000               0            0     -1          -1.39         -2.30\n",
            "173         18        0  165       100  4.185849  41.450000              15            0      7           8.56          8.51\n",
            "100    1.00\n",
            "121    0.00\n",
            "125    0.00\n",
            "128    0.00\n",
            "135    0.00\n",
            "143    0.50\n",
            "160    0.05\n",
            "162    0.10\n",
            "163    0.50\n",
            "173    0.40\n",
            "Name: smbToGive, dtype: float64\n",
            "Trained model with 3 layers and loss of 0.12461606413125992\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2023-3-9_0-37'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}