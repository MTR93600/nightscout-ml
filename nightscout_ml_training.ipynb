{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MTR93600/nightscout-ml/blob/main/nightscout_ml_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to nightscout-ml\n"
      ],
      "metadata": {
        "id": "4-Eqn-XgGKY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "data_folder = '/content/drive/My Drive/nightscout-ml/'\n",
        "data_xlsx_file = data_folder+'data.xlsx'\n",
        "path = Path(data_xlsx_file)\n",
        "if path.is_file():\n",
        "    print(\"üëç Found data file at location: \"+data_xlsx_file)\n",
        "else:\n",
        "    csv_file = data_folder+'csvfile/aiSMB_Newrecords.csv'\n",
        "    if not os.path.exists(csv_file):\n",
        "        raise Exception(\"CSV file not found in location: \"+csv_file)\n",
        "    else:\n",
        "        print(\"CSV file found at location: \"+csv_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oneIsBk3zFNb",
        "outputId": "ab0bf6f5-c476-453c-e903-deb9f1c9cb02"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "üëç Found data file at location: /content/drive/My Drive/nightscout-ml/data.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import openpyxl\n",
        "import datetime\n",
        "import warnings\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
        "\n",
        "\n",
        "\n",
        "# Lire le fichier CSV\n",
        "csv_file = '/content/drive/My Drive/nightscout-ml/csvfile/aiSMB_Newrecords.csv'\n",
        "data = pd.read_csv(csv_file)\n",
        "\n",
        "# Renommer les colonnes\n",
        "data.columns = ['dateStr', 'dateLong', 'hourOfDay', 'weekend',\n",
        "                'bg', 'targetBg', 'iob', 'cob', 'lastCarbAgeMin', 'futureCarbs', 'delta', 'shortAvgDelta', 'longAvgDelta',\n",
        "                'accelerating_up', 'deccelerating_up', 'accelerating_down', 'deccelerating_down', 'stable',\n",
        "                'tdd7DaysPerHour', 'tdd2DaysPerHour', 'tddDailyPerHour', 'tdd24HrsPerHour',\n",
        "                'recentSteps5Minutes', 'recentSteps10Minutes', 'recentSteps15Minutes', 'recentSteps30Minutes', 'recentSteps60Minutes',\n",
        "                'tags0to60minAgo', 'tags60to120minAgo', 'tags120to180minAgo', 'tags180to240minAgo',\n",
        "                'variableSensitivity', 'lastbolusage', 'predictedSMB', 'maxIob', 'maxSMB', 'smbToGive']\n",
        "\n",
        "# Appliquer la condition\n",
        "data.loc[(data['predictedSMB'] == 0) & (data['smbToGive'] > 0), 'predictedSMB'] = data['smbToGive']\n",
        "\n",
        "# Chemin du fichier Excel\n",
        "excel_file = '/content/drive/My Drive/nightscout-ml/data.xlsx'\n",
        "\n",
        "# V√©rifier si le fichier existe et le renommer si n√©cessaire\n",
        "if os.path.exists(excel_file):\n",
        "    today = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
        "    new_file_name = f'/content/drive/My Drive/nightscout-ml/data_{today}.xlsx'\n",
        "    os.rename(excel_file, new_file_name)\n",
        "\n",
        "# Cr√©er un fichier Excel et ajouter les donn√©es √† la feuille \"training_data\"\n",
        "with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:\n",
        "    data.to_excel(writer, sheet_name='training_data', index=False)\n",
        "    \n",
        "    # Cr√©er un objet PieChart\n",
        "    chart = openpyxl.chart.PieChart()\n",
        "    \n",
        "    # Cr√©er une nouvelle feuille pour le temps dans la cible\n",
        "    workbook = writer.book\n",
        "    tir_sheet = workbook.create_sheet('TIR')\n",
        "\n",
        "    # Calculer le temps pass√© dans la cible\n",
        "    bg_range = data['bg'].tolist()\n",
        "    tir_count = len([x for x in bg_range if 65 <= x <= 180])\n",
        "    total_count = len(bg_range)\n",
        "    tir_percentage = round(tir_count / total_count * 100, 2)  # arrondir √† 2 d√©cimales\n",
        "\n",
        "    date_format_str = '%m/%d/%y %I:%M%p'\n",
        "\n",
        "    def time_to_str(date):\n",
        "        date_str = datetime.strftime(date, date_format_str)\n",
        "        \n",
        "        # drop leading 0 on month\n",
        "        date_str = date_str[1:] if date_str.startswith('0') else date_str\n",
        "        \n",
        "        date_str = date_str.replace('/0', '/')\n",
        "        return date_str\n",
        "\n",
        "    \n",
        "    # Obtenir la date actuelle\n",
        "    today = datetime.date.today()\n",
        "    \n",
        "    # R√©cup√©rer la premi√®re et la derni√®re date\n",
        "    first_date = datetime.datetime.strptime(data['dateStr'].iloc[0], '%m/%d/%y %H:%M').date()\n",
        "    \n",
        "\n",
        "    # Calculer le nombre de jours entre les deux dates\n",
        "    nb_jours = (today - first_date).days\n",
        "    \n",
        "    # Ajouter le pourcentage et le nombre de jours √† la feuille \"TIR\"\n",
        "    tir_sheet.cell(row=1, column=1, value=\"TIR\")\n",
        "    tir_sheet.cell(row=1, column=2, value=\"%\")\n",
        "    tir_sheet.cell(row=1, column=3, value=\"NB DAYS\")\n",
        "    tir_sheet.cell(row=2, column=1, value=\"65-180\")\n",
        "    tir_sheet.cell(row=2, column=2, value=tir_percentage)\n",
        "    tir_sheet.cell(row=2, column=3, value=nb_jours)\n",
        "    # Calculer le temps pass√© dans la cible\n",
        "    bg_range = data['bg'].tolist()\n",
        "    tir_count = len([x for x in bg_range if 65 <= x <= 180])\n",
        "    total_count = len(bg_range)\n",
        "    tir_percentage = round(tir_count / total_count * 100, 2)  # arrondir √† 2 d√©cimales\n",
        "\n",
        "    # D√©finir la variable `sizes`\n",
        "    sizes = [tir_count, total_count - tir_count]\n",
        "    # D√©finir les √©tiquettes\n",
        "    labels = ['Temps pass√© dans la cible (65-180)', 'Temps pass√© hors de la cible']\n",
        "    colors = ['#008000', '#FFFF00']\n",
        "    # Cr√©er un diagramme circulaire avec des couleurs personnalis√©es\n",
        "    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "\n",
        "    # Ajouter un titre\n",
        "    plt.title(f\"TIR (65-180) period : {nb_jours} DAYS\")\n",
        "\n",
        "    # Enregistrer le graphique comme image\n",
        "    plt.savefig(\"/content/drive/My Drive/nightscout-ml/graphique.png\")\n",
        "    plt.clf()\n",
        "\n",
        "    # Afficher le graphique\n",
        "    #plt.show()\n",
        "\n",
        "    # Ajouter le graphique √† la feuille \"TIR\"\n",
        "    from openpyxl.drawing.image import Image\n",
        "    \n",
        "    cwd = os.getcwd()\n",
        "    chart_file = os.path.join(cwd, \"/content/drive/My Drive/nightscout-ml/graphique.png\")\n",
        "    img = Image(chart_file)\n",
        "    tir_sheet.add_image(img, \"D2\")\n",
        "    \n",
        "    # Sauvegarder le fichier Excel\n",
        "    writer.save()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "Acz-zmNF2EQJ",
        "outputId": "d6b0075a-5cf7-46cb-bbef-92caa124813e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-8f5aac0ec872>:115: FutureWarning: save is not part of the public API, usage can give unexpected results and will be removed in a future version\n",
            "  writer.save()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "date_format_str = '%m/%d/%y %I:%M%p'\n",
        "\n",
        "def time_to_str(date):\n",
        "    date_str = datetime.strftime(date, date_format_str)\n",
        "    \n",
        "    # drop leading 0 on month\n",
        "    date_str = date_str[1:] if date_str.startswith('0') else date_str\n",
        "    \n",
        "    date_str = date_str.replace('/0', '/')\n",
        "    return date_str\n",
        "\n",
        "def str_to_time(date_str):\n",
        "    return datetime.strptime(date_str, date_format_str)"
      ],
      "metadata": {
        "id": "z5Et-tfKKNCZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from datetime import datetime\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import PReLU\n",
        "import time\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "pd.set_option('display.width', 3000)\n",
        "now = datetime.now()\n",
        "date_str = \"{}-{}-{}_{}-{}\".format(now.year, now.month, now.day, now.hour, now.minute)\n",
        "time_ranges = ['0to60minAgo', '60to120minAgo', '120to180minAgo', '180to240minAgo']\n",
        "log_folder = 'logs'\n",
        "\n",
        "ISF = [\"variableSensitivity\"]\n",
        "time_bolus = [\"lastbolusage\"]\n",
        "hour_breakdowns = [\"hour0_2\",\"hour3_5\",\"hour6_8\",\"hour9_11\",\"hour12_14\",\"hour15_17\",\"hour18_20\",\"hour21_23\"]\n",
        "accellerating = [\"accelerating_up\",\"deccelerating_up\",\"accelerating_down\",\"deccelerating_down\",\"stable\"]\n",
        "recent_steps = [\"recentSteps5Minutes\",\"recentSteps10Minutes\",\"recentSteps15Minutes\",\"recentSteps30Minutes\",\"recentSteps60Minutes\"]\n",
        "sleep_seated = [\"sleep\",\"sedentary\"]\n",
        "tdd = [\"tdd7Days\",\"tddDaily\",\"tdd24Hrs\"]\n",
        "tddPerHour = [\"tdd7DaysPerHour\",\"tdd2DaysPerHour\",\"tddDailyPerHour\",\"tdd24HrsPerHour\"]\n",
        "base_cols = [\n",
        "                \"hourOfDay\",\"weekend\",\"bg\",\n",
        "             #\"targetBg\",\n",
        "              \"iob\",\n",
        "             #\"cob\",\"lastCarbAgeMin\",\"futureCarbs\",\n",
        "                #\"bg\",\"targetBg\",\"iob\",\n",
        "                \"delta\",\"shortAvgDelta\",\"longAvgDelta\"]\n",
        "cob_delta = ['cobDelta']\n",
        "gi_tags = ['giFast', 'giMedium', 'giSlow']\n",
        "prior_bgs_to_add = 36\n",
        "\n",
        "col_map = {\n",
        "    'best_cols': base_cols+tddPerHour,\n",
        "    #'best_cols': base_cols+tddPerHour+recent_steps,\n",
        "    # 'best_cols_cob_delta': base_cols+tddPerHour+cob_delta+recent_steps+cob_delta,\n",
        "    # 'best_cols_plus_glycemic_index': base_cols+tddPerHour+recent_steps+gi_tags,\n",
        "    # 'best_cols_plus_fast_med_slow': base_cols+tddPerHour+recent_steps+['fastCarbs','mediumCarbs','slowCarbs']\n",
        "    # 'best_cols_plus_prior_bgs_and_smbs': base_cols+tddPerHour\n",
        "    # 'best_cols_w_accell': base_cols+tddPerHour+cob_delta+recent_steps+accellerating,\n",
        "    \n",
        "    ### meal tags didn't improve model accuracy sadly :(\n",
        "    # 'best_cols_plus_tags': base_cols+tddPerHour+cob_delta+recent_steps\n",
        "\n",
        "    #'base': base_cols, # comment this line\n",
        "    # 'base_cobDelta': base_cols+cob_delta, # helps a little\n",
        "    # 'base_tddPerhour': base_cols+tddPerHour, # helpful\n",
        "    # 'base_recentSteps': base_cols+recent_steps, # helpful\n",
        "    # 'base_accellerating': base_cols+accellerating, # helpful, but could probably refine\n",
        "    \n",
        "    # 'base_tdd': base_cols+tdd, # helps, but tddPerHour values are more effective\n",
        "    # 'base_tdd_tddPerHour': base_cols+tdd+tddPerHour, # not more helpful than tddPerHour\n",
        "    # 'base_recentSteps_sleepSeated': base_cols+recent_steps+sleep_seated, # no major diff, steps is fine\n",
        "    # 'base_sleepSeated': base_cols+sleep_seated, # no major diff thus drop\n",
        "    # 'base_hour_breakdowns': base_cols+hour_breakdowns # not helpful thus drop\n",
        "}\n",
        "\n",
        "# Define model hyper parameters such as input values and nural network dimensions\n",
        "HP_COLS = hp.HParam('cols', hp.Discrete(list(col_map.keys())))\n",
        "\n",
        "HP_NUM_NODES_L1 = hp.HParam('num_nodes_l1', hp.Discrete([5])) \n",
        "HP_NUM_NODES_L2 = hp.HParam('num_nodes_l2', hp.Discrete([0]))\n",
        "HP_NUM_NODES_L3 = hp.HParam('num_nodes_l3', hp.Discrete([0]))\n",
        "HP_NUM_NODES_L4 = hp.HParam('num_nodes_l4', hp.Discrete([0]))\n",
        "HP_NUM_EPOCHS = hp.HParam('num_epochs', hp.Discrete([2]))\n",
        "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([.1]))\n",
        "HP_LAST_ACTIVATION = hp.HParam('last_activation', hp.Discrete(['prelu']))\n",
        "METRIC_LOSS = 'loss'\n",
        "\n",
        "def build_model(run_dir, hparams, train_features, train_labels, test_features, test_labels):\n",
        "    loss = 1\n",
        "    model = ''\n",
        "    with tf.summary.create_file_writer(run_dir).as_default():\n",
        "        hp.hparams(hparams)  # record the values used in this trial\n",
        "        model, loss = train_test_model(hparams, train_features, train_labels, test_features, test_labels)\n",
        "        tf.summary.scalar(METRIC_LOSS, loss, step=1)\n",
        "    return model, loss\n",
        "\n",
        "def train_test_model(hparams, train_features, train_labels, test_features, test_labels):\n",
        "    normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "    normalizer.adapt(np.array(train_features))\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Input(shape=(train_features.shape[1],)))\n",
        "    model.add(normalizer)\n",
        "    if hparams[HP_NUM_NODES_L1] > 0:\n",
        "        model.add(layers.Dense(units=hparams[HP_NUM_NODES_L1], activation=\"relu\"))\n",
        "    if hparams[HP_NUM_NODES_L2] > 0:\n",
        "        model.add(layers.Dense(units=hparams[HP_NUM_NODES_L2], activation=\"relu\"))\n",
        "    if hparams[HP_NUM_NODES_L3] > 0 and hparams[HP_NUM_NODES_L2] > 0:\n",
        "        model.add(layers.Dense(units=hparams[HP_NUM_NODES_L3], activation=\"relu\"))\n",
        "    if hparams[HP_NUM_NODES_L4] > 0 and hparams[HP_NUM_NODES_L3] > 0 and hparams[HP_NUM_NODES_L2] > 0:\n",
        "        model.add(layers.Dense(units=hparams[HP_NUM_NODES_L4], activation=\"relu\"))\n",
        "\n",
        "    if hparams[HP_LAST_ACTIVATION] == 'prelu':\n",
        "        prelu = PReLU()\n",
        "        model.add(layers.Dense(units=1, activation=prelu))\n",
        "    else:\n",
        "        model.add(layers.Dense(units=1, activation=hparams[HP_LAST_ACTIVATION]))\n",
        "\n",
        "    model.compile(\n",
        "        # optimizer=tf.optimizers.Adam(learning_rate=hparams[HP_LEARNING_RATE]),\n",
        "        optimizer='adam',\n",
        "        loss='mean_absolute_error')\n",
        "\n",
        "    model.fit(\n",
        "        train_features,\n",
        "        train_labels,\n",
        "        epochs=hparams[HP_NUM_EPOCHS],\n",
        "        # Suppress logging.\n",
        "        verbose=1,\n",
        "        # Calculate validation results on 20% of the training data.\n",
        "        validation_split = 0.2)\n",
        "    \n",
        "    loss = model.evaluate(test_features, test_labels)\n",
        "    print(train_features.head(10))\n",
        "    print(train_labels.head(10))\n",
        "    print(test_features.head(10))\n",
        "    print(test_labels.head(10))\n",
        "    print(f\"Trained model with {len(model.layers)} layers and loss of {loss}\")\n",
        "    return model, loss\n",
        "\n",
        "def add_glycemic_index_cols(df):\n",
        "    # load vocabulary map from csv\n",
        "    df_gi = pd.read_csv('glycemic_index.csv')\n",
        "    for index, row in df_gi.iterrows():\n",
        "        tag = row['tag']\n",
        "        gi = row['index']\n",
        "        \n",
        "        # if 0-60 min contains fast acting carb word, then set fast GI to 1\n",
        "        if gi == 'fast':\n",
        "            df['giFast'] = df['tags0to60minAgo'].str.contains(tag)\n",
        "            df['giFast'] = df['giFast'].astype(int)\n",
        "        \n",
        "        # if 0-60 or 60-120 contains medium acting carb word, then set medium GI col to 1\n",
        "        if gi == 'medium':\n",
        "            df['giMedium'] = df['tags0to60minAgo'].str.contains(tag)\n",
        "            df['giMedium'] = df['tags60to120minAgo'].str.contains(tag)\n",
        "            df['giMedium'] = df['giMedium'].astype(int)\n",
        "        \n",
        "        # if tags contain slow acting word, set slow GI column to 1\n",
        "        if gi == 'slow':\n",
        "            df['giSlow'] = df['tags60to120minAgo'].str.contains(tag)\n",
        "            df['giSlow'] = df['tags120to180minAgo'].str.contains(tag)\n",
        "            df['giSlow'] = df['tags180to240minAgo'].str.contains(tag)\n",
        "            df['giSlow'] = df['giSlow'].astype(int)\n",
        "        \n",
        "    return df\n",
        "\n",
        "def convert_tags_to_cols(df):\n",
        "    vocabulary = get_vocabulary_from_tags(df)\n",
        "    \n",
        "    # fill in NaN values with empty strings to support setting tags\n",
        "    for range in time_ranges:\n",
        "        df['tags'+range] = df['tags'+range].fillna('')\n",
        "\n",
        "    for word in vocabulary:\n",
        "        for range in time_ranges:\n",
        "            col_name = word+'_'+range\n",
        "            df[col_name] = df['tags'+range].str.contains(word)\n",
        "            df[col_name] = df[col_name].astype(int)\n",
        "    return df, vocabulary\n",
        "\n",
        "def get_vocabulary_from_tags(df):\n",
        "    # get list of unique words\n",
        "    words = list(df['tags0to60minAgo'].str.split(' ', expand=True).stack().unique())\n",
        "    stop_words = ['for', 'a', 'an', 'and', 'of', 'with', '']\n",
        "    \n",
        "    # vocaublary = [word.lower() for word in vocaublary]\n",
        "    vocabulary = []\n",
        "    for word in words:\n",
        "        word = word.lower()\n",
        "        word = word[:-2] if word.endswith(\"es\") else word\n",
        "        word = word[:-1] if word.endswith(\"s\") else word\n",
        "        if word not in stop_words and word not in vocabulary:\n",
        "            vocabulary.append(word)\n",
        "    # strip commas, strip stop words (and, a) - android strips a/an/and\n",
        "    # stem words if need be\n",
        "    return vocabulary\n",
        "\n",
        "def get_basic_model_desc(model):\n",
        "    model_info = \"\\n\\n------------\\n\"\n",
        "    model_info += f\"Model {date_str}\\n\"\n",
        "    model_info += f\"{len(model.layers)} Layers:\\n\"\n",
        "    for i in range (len(model.layers)):\n",
        "        layer = model.layers[i]\n",
        "        layer_info = f\"    - Layer {i}  - {layer.name}\"\n",
        "        layer_info += f\" ({layer.units})\" if 'dense' in layer.name else \"\"\n",
        "        layer_info += f\" ({layer.rate})\" if 'dropout' in layer.name else \"\"\n",
        "        model_info += layer_info + \"\\n\"\n",
        "    return model_info\n",
        "\n",
        "def save_model_info_v2(model, cols, best_loss, num_epochs, data_row_count, training_time, best_last_activation, best_learning_rate, vocabulary):\n",
        "    model_info = get_basic_model_desc(model)\n",
        "\n",
        "    model_info += f\"Model Loss & Accuracy: {str(round(best_loss, 5))}\\n\"\n",
        "    model_info += f\"Number of Epochs: {num_epochs} \\n\"\n",
        "    model_info += f\"Columns ({len(cols)}): {cols} - {col_map[cols]} \\n\"\n",
        "    model_info += f\"Vocabulary ({len(vocabulary)}) {vocabulary}\\n\"\n",
        "    model_info += f\"Training Data Size: {data_row_count} \\n\"\n",
        "    model_info += f\"Learning Rate: {best_learning_rate} \\n\"\n",
        "    model_info += f\"Activation: {best_last_activation} \\n\" if best_last_activation is not None else \"Activation: None\\n\"\n",
        "    # model_info += basic_predictions(model) + \"\\n\"\n",
        "    model_info += f\"Took {time.strftime('%H:%M:%S', time.gmtime(training_time))} to train\\n\"\n",
        "    model_info += \"NOTES: \\n\"\n",
        "    open(data_folder+'/models/tf_model_results.txt', \"a\").write(model_info)\n",
        "\n",
        "\n",
        "\n",
        "def basic_predictions(model):\n",
        "    if len(current_cols) != 38:\n",
        "        return f\"ERROR: incorrect number of columns ({len(current_cols)})\"\n",
        "\n",
        "    low = basic_predict(model,50,0.0,0.0,0)\n",
        "    low_w_iob = basic_predict(model,50,1.0,0.0,0)\n",
        "    normal_w_iob = basic_predict(model,100,1.0,0.0,0)\n",
        "    normal_wo_iob = basic_predict(model,100,0.0,0.0,0)\n",
        "    high_bg = basic_predict(model,200,0.0,0.0,0)\n",
        "    high_cob = basic_predict(model,100,1.0,30.0,0)\n",
        "    high_both = basic_predict(model,200,1.0,30.0,0)\n",
        "    line =  f\"    low: {low}    low_w_iob: {low_w_iob}    normal_w_iob: {normal_w_iob}    normal_wo_iob: {normal_wo_iob}\\n\"\n",
        "    line += f\"    high_bg: {high_bg}    high_cob: {high_cob}    high_both: {high_both}    high_both_and_rising {basic_predict(model, 200, 1, 60, 10)}\\n\"\n",
        "    line += f\"    low_rising  : {basic_predict(model, 70, 0, 20, 10)}    normal_rising  : {basic_predict(model, 100, 0, 20, 10)}    high_rising  : {basic_predict(model, 180, 0, 20, 10)}\\n\"\n",
        "    line += f\"    low_dropping: {basic_predict(model, 70, 2, 0, -7)}    normal_dropping: {basic_predict(model, 100, 2, 0, -7)}    high_dropping: {basic_predict(model, 180, 2, 0, -7)}\\n\"\n",
        "    return line\n",
        "    \n",
        "def basic_predict(model, bg, iob, cob, delta):\n",
        "    last_cob_min = 0 if cob == 0 else 5\n",
        "    accelerating_up = 1 if delta > 3 else 0\n",
        "    deccelerating_down = 1 if delta < -3 else 0\n",
        "    stable= 1 if delta > -3 and delta < 3 else 0\n",
        "    prediction = model.predict([[11,1,0,0, 0,0,0, 0,0,0, \n",
        "                    bg,100,iob,cob,last_cob_min,0,delta,delta,delta,\n",
        "                    accelerating_up, 0, deccelerating_down, 0, stable, \n",
        "                    33,1, 33,1, 33,1,\n",
        "                    0,0,0, 0,0,\n",
        "                    0,1]])\n",
        "                    # \"hourOfDay\",\"hour0_2\",\"hour3_5\",\"hour6_8\", \"hour9_11\",\"hour12_14\",\"hour15_17\", \"hour18_20\",\"hour21_23\",\"weekend\",\n",
        "                    # \"bg\",\"targetBg\",\"iob\",\"cob\",\"lastCarbAgeMin\",\"futureCarbs\",\"delta\",\"shortAvgDelta\",\"longAvgDelta\",\n",
        "                    # \"accelerating_up\",\"deccelerating_up\",\"accelerating_down\",\"deccelerating_down\",\"stable\",\n",
        "                    # \"tdd7Days\",\"tddDaily\",\"tddPerHour\",\"tdd24Hrs\",\n",
        "                    # \"tdd7Days\",\"tdd7DaysPerHour\", \"tddDaily\",\"tddDailyPerHour\", \"tdd24Hrs\",\"tdd24HrsPerHour\",\n",
        "                    # \"recentSteps5Minutes\",\"recentSteps10Minutes\",\"recentSteps15Minutes\", \"recentSteps30Minutes\",\"recentSteps60Minutes\",\n",
        "                    # \"sleep\",\"sedintary\",\n",
        "                    \n",
        "    return str(round(prediction[0][0], 3))\n",
        "\n",
        "def clear_log_folder():\n",
        "    if os.path.exists(log_folder):\n",
        "        shutil.rmtree(log_folder)\n",
        "    os.mkdir(log_folder)\n",
        "    if not os.path.exists(data_folder+'/models'):\n",
        "        os.mkdir(data_folder+'/models')\n",
        "\n",
        "\n",
        "\n",
        "def save_model(model):\n",
        "    # https://www.tensorflow.org/tutorials/keras/save_and_load#savedmodel_format\n",
        "    model.save(data_folder+'/models/backup/tf_model_'+date_str)\n",
        "\n",
        "    # https://medium.com/analytics-vidhya/running-ml-models-in-android-using-tensorflow-lite-e549209287f0\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model=model)\n",
        "    lite_model = converter.convert()\n",
        "    open(data_folder+'/models/backup/tf_model_'+date_str+'.tflite', \"wb\").write(lite_model)\n",
        "    open(data_folder+'/models/model.tflite', \"wb\").write(lite_model)\n",
        "\n",
        "\n",
        "def compare_two_models(model_1_date, model_2_date, row_dates):\n",
        "    m1 = tf.keras.models.load_model(f'{data_folder}/models/backup/tf_model_{model_1_date}')\n",
        "    m2 = tf.keras.models.load_model(f'{data_folder}/models/backup/tf_model_{model_2_date}')\n",
        "\n",
        "    df = pd.read_excel(data_xlsx_file,'training_data')\n",
        "    \n",
        "    # row_1 = row.copy()\n",
        "    # row_1.pop('tdd7DaysPerHour')\n",
        "    # row_1.pop('tdd24HrsPerHour')\n",
        "\n",
        "    eval_features = df.sample(frac=0.3, random_state=0)\n",
        "    eval_features = eval_features[current_cols]\n",
        "    eval_features_1 = eval_features.copy()\n",
        "    # eval_features_1.pop('tdd7DaysPerHour')\n",
        "    # eval_features_1.pop('tdd24HrsPerHour')\n",
        "    eval_features_1.pop('smbToGive')\n",
        "    eval_labels = eval_features.pop('smbToGive')\n",
        "\n",
        "    m1_eval = str(round(m1.evaluate(eval_features_1, eval_labels)[0], 6))\n",
        "    m2_eval = str(round(m2.evaluate(eval_features, eval_labels)[0], 6))\n",
        "    eval_diff = str(round((float(m2_eval) - float(m1_eval)), 5))\n",
        "\n",
        "    # m1_predict = str(round(m1.predict([row_1])[0][0],3))\n",
        "    # m2_predict = str(round(m2.predict([row])[0][0],3))\n",
        "\n",
        "    model_info = \" ---- Model Comparison ----\\n\"\n",
        "    model_info += f\" ---- MODELS: model 1: {model_1_date} - model 2: {model_2_date}\\n\"\n",
        "    model_info += f\" ---- EVAL LOSS:   model 1: {m1_eval} - model 2: {m2_eval} => \"\n",
        "    model_info += f\"{(eval_diff)} getting better \\n\\n\" if float(eval_diff) < 0 else f\"{(eval_diff)} !!! BAD !!! \\n\\n\"\n",
        "    for row_date in row_dates:\n",
        "        row = df.loc[df['dateStr'] == row_date]\n",
        "        row = row[current_cols]\n",
        "        smb_to_give = row['smbToGive'].values[0]\n",
        "        row.pop('smbToGive')\n",
        "        m1_predict = str(round(m1.predict([row])[0][0],3))\n",
        "        m2_predict = str(round(m2.predict([row])[0][0],3))\n",
        "        model_info += f\" ------ PREDICTIONS: {row_date} \"\n",
        "        model_info += f\"({smb_to_give}u, {row['bg'].values[0]}mg/dL, delta: {row['delta'].values[0]}, shortAvgDelta: {row['shortAvgDelta'].values[0]})\"\n",
        "        model_info += f\" - model 1: {m1_predict} - model 2: {m2_predict}\\n\"\n",
        "    \n",
        "    open(data_folder+'/models/tf_model_results.txt', \"a\").write(model_info)\n",
        "\n",
        "\n",
        "\n",
        "def build_tf_regression():\n",
        "    start = time.time()\n",
        "    df = pd.read_excel(data_xlsx_file,'training_data')\n",
        "    # df = df[100:9566]\n",
        "    clear_log_folder()\n",
        "\n",
        "    with tf.summary.create_file_writer(f'{log_folder}/hparam_tuning').as_default():\n",
        "        hp.hparams_config(\n",
        "            hparams=[HP_COLS, HP_NUM_NODES_L1, HP_NUM_NODES_L2, HP_NUM_NODES_L3, HP_NUM_NODES_L4, \n",
        "            HP_NUM_EPOCHS, HP_LEARNING_RATE, HP_LAST_ACTIVATION],\n",
        "            metrics=[hp.Metric(METRIC_LOSS, display_name='loss')],\n",
        "        )\n",
        "\n",
        "    if 'best_cols_plus_glycemic_index' in col_map.keys() or 'best_cols_plus_tags' in col_map.keys():\n",
        "        df, vocabulary = convert_tags_to_cols(df)\n",
        "        if 'best_cols_plus_glycemic_index' in col_map.keys():\n",
        "            df = add_glycemic_index_cols(df)\n",
        "    else:\n",
        "        vocabulary = ['not using tags']\n",
        "    train_dataset = df.sample(frac=0.8, random_state=0)\n",
        "    test_dataset = df.drop(train_dataset.index)\n",
        "\n",
        "    train_features = train_dataset.copy()\n",
        "    test_features = test_dataset.copy()\n",
        "\n",
        "    train_labels = train_features.pop('smbToGive')\n",
        "    test_labels = test_features.pop('smbToGive')\n",
        "\n",
        "    best_loss = 1\n",
        "    best_model = 1\n",
        "    best_epochs = 0\n",
        "    best_last_activation = ''\n",
        "    best_learning_rate = .1\n",
        "    best_cols = ''\n",
        "\n",
        "    \n",
        "    \n",
        "    session_num = 0\n",
        "\n",
        "    for cols_name in HP_COLS.domain.values:\n",
        "        columns = col_map[cols_name]\n",
        "        if 'plus_tags' in cols_name:\n",
        "            for time_range in time_ranges:\n",
        "                for word in vocabulary:\n",
        "                    columns.append(f\"{word}_{time_range}\")\n",
        "        if 'prior_bgs_and_smbs' in cols_name:\n",
        "            for offset in range(0, prior_bgs_to_add):\n",
        "                offset_in_5_min = (offset+1) * 5\n",
        "                columns.append(f\"{offset_in_5_min}_min_prior_bg\")\n",
        "                columns.append(f\"{offset_in_5_min}_min_prior_smb\")\n",
        "\n",
        "        iteration_train_features = train_features[columns]\n",
        "        iteration_test_features = test_features[columns]\n",
        "        for num_nodes_l1 in HP_NUM_NODES_L1.domain.values:\n",
        "            for num_nodes_l2 in HP_NUM_NODES_L2.domain.values:\n",
        "                for num_nodes_l3 in HP_NUM_NODES_L3.domain.values:\n",
        "                    for num_nodes_l4 in HP_NUM_NODES_L4.domain.values:\n",
        "                        for num_epochs in HP_NUM_EPOCHS.domain.values:\n",
        "                            for learn_rate in HP_LEARNING_RATE.domain.values:\n",
        "                                for last_activation in HP_LAST_ACTIVATION.domain.values:\n",
        "                                    hparams = {\n",
        "                                        HP_COLS: cols_name,\n",
        "                                        HP_NUM_NODES_L1: num_nodes_l1,\n",
        "                                        HP_NUM_NODES_L2: num_nodes_l2,\n",
        "                                        HP_NUM_NODES_L3: num_nodes_l3,\n",
        "                                        HP_NUM_NODES_L4: num_nodes_l4,\n",
        "                                        HP_NUM_EPOCHS: num_epochs,\n",
        "                                        HP_LEARNING_RATE: learn_rate,\n",
        "                                        HP_LAST_ACTIVATION: last_activation\n",
        "                                    }\n",
        "                                    run_name = f\"run-{cols_name}-{session_num}\"\n",
        "                                    print(f\"--- Starting trail {run_name}\")\n",
        "                                    print({h.name: hparams[h] for h in hparams})\n",
        "                                    model, loss = build_model('logs/hparam_tuning/' + run_name, hparams, iteration_train_features, train_labels, iteration_test_features, test_labels)\n",
        "                                    session_num += 1\n",
        "                                    if loss < best_loss:\n",
        "                                        best_loss = loss\n",
        "                                        best_model = model\n",
        "                                        best_epochs = num_epochs\n",
        "                                        best_learning_rate = learn_rate\n",
        "                                        best_last_activation = last_activation\n",
        "                                        best_cols = cols_name\n",
        "    \n",
        "    training_time = time.time() - start\n",
        "    \n",
        "    save_model_info_v2(best_model, best_cols, best_loss, best_epochs, len(df), training_time, best_last_activation, best_learning_rate, vocabulary)\n",
        "\n",
        "    save_model(best_model)\n",
        "    \n",
        "    return date_str\n",
        "\n",
        "build_tf_regression()"
      ],
      "metadata": {
        "id": "1kxMrMw0Kdwf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        },
        "outputId": "95dfa290-dd36-42f0-bff0-1c289b4f3c44"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting trail run-best_cols-0\n",
            "{'cols': 'best_cols', 'num_nodes_l1': 5, 'num_nodes_l2': 0, 'num_nodes_l3': 0, 'num_nodes_l4': 0, 'num_epochs': 2, 'learning_rate': 0.1, 'last_activation': 'prelu'}\n",
            "Epoch 1/2\n",
            "51/51 [==============================] - 1s 5ms/step - loss: 0.6618 - val_loss: 0.5281\n",
            "Epoch 2/2\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4006 - val_loss: 0.3117\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3085\n",
            "      hourOfDay  weekend   bg       iob  delta  shortAvgDelta  longAvgDelta  tdd7DaysPerHour  tdd2DaysPerHour  tddDailyPerHour  tdd24HrsPerHour\n",
            "840           8        0  110  0.275711     -1           0.28          2.45         1.396513         1.287934         1.361055         1.389451\n",
            "1264         20        0  110  4.743515      0          -1.00         -1.33         1.381209         1.275562         1.375681         1.495174\n",
            "575           6        1  103  0.338183      2           2.50          2.16         1.403230         1.248451         1.214813         1.351042\n",
            "944          17        0   83  0.195537      1           1.22          1.11         1.396513         1.287934         1.361055         1.289285\n",
            "2275         17        0   93  1.748270     -6          -5.39         -4.55         1.183275         1.311653         1.313986         1.465458\n",
            "923          15        0   95  1.519672      2           1.39         -0.19         1.396513         1.287934         1.361055         1.367465\n",
            "892          13        0  121  2.066277     -1          -0.50          0.73         1.396513         1.287934         1.361055         1.351979\n",
            "1872         22        1  160  0.870214      9           9.89         10.45         1.170874         1.176677         1.142667         1.241153\n",
            "1560         17        0  162  2.386022     12          11.50          6.30         1.191381         0.965292         0.870521         0.955875\n",
            "1610          8        1   90  0.387728      0          -0.39         -1.36         1.181181         1.040604         1.210687         1.161403\n",
            "840     0.0\n",
            "1264    0.0\n",
            "575     0.0\n",
            "944     0.0\n",
            "2275    0.0\n",
            "923     0.0\n",
            "892     0.0\n",
            "1872    0.0\n",
            "1560    1.0\n",
            "1610    0.0\n",
            "Name: smbToGive, dtype: float64\n",
            "    hourOfDay  weekend   bg       iob  delta  shortAvgDelta  longAvgDelta  tdd7DaysPerHour  tdd2DaysPerHour  tddDailyPerHour  tdd24HrsPerHour\n",
            "0          22        0  204  7.634711      7           5.39          6.26          1.37953         1.480854         1.579382         1.624792\n",
            "3          22        0  215  8.676094      0           2.17          4.40          1.37953         1.480854         1.579382         1.689722\n",
            "7          23        0  178  6.918586    -19         -15.39         -5.67          1.37953         1.480854         1.579382         1.679653\n",
            "21          1        0  163  1.028999     -2          -2.28         -1.57          1.44789         1.629691         1.680000         1.633222\n",
            "24          1        0  169  0.788427      5           3.50          0.05          1.44789         1.629691         1.680000         1.637180\n",
            "25          1        0  174  1.017405      6           5.00          1.30          1.44789         1.629691         1.680000         1.649681\n",
            "26          1        0  175  1.944006      2           3.06          1.79          1.44789         1.629691         1.680000         1.691347\n",
            "63          5        0   62  0.082786      3           2.89          1.72          1.44789         1.629691         1.680000         1.645826\n",
            "67          5        0   75  0.056836      3           3.28          3.07          1.44789         1.629691         1.680000         1.652951\n",
            "84          6        0   68  0.010972     -3          -3.00         -2.86          1.44789         1.629691         1.680000         1.659680\n",
            "0     0.0\n",
            "3     0.0\n",
            "7     0.0\n",
            "21    0.0\n",
            "24    0.3\n",
            "25    1.0\n",
            "26    0.0\n",
            "63    0.0\n",
            "67    0.0\n",
            "84    0.0\n",
            "Name: smbToGive, dtype: float64\n",
            "Trained model with 3 layers and loss of 0.30851125717163086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2023-4-12_14-42'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing Model\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import shutil\n",
        "import datetime\n",
        "source_folder = \"/content/drive/My Drive/nightscout-ml/models\"\n",
        "destination_folder = \"/content/drive/My Drive/nightscout-ml\"\n",
        "old_model_file_path = os.path.join(destination_folder, \"model-2.tflite\")\n",
        "new_model_file_path = os.path.join(source_folder, \"model.tflite\")\n",
        "\n",
        "# V√©rifier si l'ancien fichier model-2.tflite existe\n",
        "if os.path.isfile(old_model_file_path):\n",
        "    # Ajouter la date √† l'ancien fichier model-2.tflite\n",
        "    date_str = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
        "    renamed_old_model_file_path = os.path.join(destination_folder, f\"model-2_{date_str}.tflite\")\n",
        "    os.rename(old_model_file_path, renamed_old_model_file_path)\n",
        "    \n",
        "# Copier le fichier model.tflite vers le dossier de destination et le renommer en model-2.tflite\n",
        "shutil.copy(new_model_file_path, old_model_file_path)\n",
        "\n",
        "file_path = f'{data_folder}/model-2.tflite'\n",
        "if not Path(file_path).is_file():\n",
        "  raise Exception(\"Data file not found in location: \"+data_xlsx_file)\n",
        "\n",
        "# Load TFLite model and allocate tensors.\n",
        "interpreter = tf.lite.Interpreter(model_path=file_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "print(input_details)\n",
        "output_details = interpreter.get_output_details()\n",
        "print(output_details)\n",
        "\n",
        "def run_prediction(input_data):\n",
        "  # https://stackoverflow.com/questions/50443411/how-to-load-a-tflite-model-in-script\n",
        "  interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "  interpreter.invoke()\n",
        "  # The function `get_tensor()` returns a copy of the tensor data.\n",
        "  # Use `tensor()` in order to get a pointer to the tensor.\n",
        "  output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "  return output_data\n",
        "\n",
        "\n",
        "# Test model on random input data.\n",
        "input_shape = input_details[0]['shape']\n",
        "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
        "print(\"prediction with random data: \"+str(run_prediction(input_data)))\n",
        "\n",
        "# input columns: \n",
        "#   'hourOfDay', 'weekend', 'bg', 'targetBg', 'iob', 'cob', 'lastCarbAgeMin', 'futureCarbs', \n",
        "#   'delta', 'shortAvgDelta', 'longAvgDelta', \n",
        "#   'accelerating_up',\"'deccelerating_up','accelerating_down','deccelerating_down','stable'\n",
        "#   'variableSensitivity','lastbolusage'\n",
        "#   'tdd7DaysPerHour', 'tdd2DaysPerHour', 'tddDailyPerHour', 'tdd24HrsPerHour', \n",
        "#   'recentSteps5Minutes', 'recentSteps10Minutes', 'recentSteps15Minutes', 'recentSteps30Minutes', 'recentSteps60Minutes'\n",
        "input_data = [[\n",
        "              1, 0, 200, \n",
        "               #90, \n",
        "               1,\n",
        "               10, 10, 2,\n",
        "               #1,0, 0, 0, 0,\n",
        "               #1,\n",
        "               #50,\n",
        "               #100,\n",
        "               1.5, 1.5, 1.5, 1.5,\n",
        "               #0, 0, 0, 0, 0\n",
        "               ]]\n",
        "for i in range(len(input_data[0])):\n",
        "  # model requires float 32 numbers\n",
        "  input_data[0][i] = np.float32(input_data[0][i])\n",
        "\n",
        "print(\"prediction with input data: \"+str(run_prediction(input_data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7BtzaJNYuvg",
        "outputId": "1208b4a4-9037-47dd-8939-3087e4e9a152"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'name': 'serving_default_input_3:0', 'index': 0, 'shape': array([ 1, 11], dtype=int32), 'shape_signature': array([-1, 11], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "[{'name': 'StatefulPartitionedCall:0', 'index': 12, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([-1,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "prediction with random data: [[1.2724128]]\n",
            "prediction with input data: [[0.50717187]]\n"
          ]
        }
      ]
    }
  ]
}