{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MTR93600/nightscout-ml/blob/main/nightscout_ml_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to AIMI-AI-MODEL\n"
      ],
      "metadata": {
        "id": "4-Eqn-XgGKY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#first step connect to your drive and create the necesserary folders\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "data_folder = '/content/drive/My Drive/nightscout-ml/'\n",
        "csv_folder = data_folder + 'csvfile/'\n",
        "data_xlsx_file = data_folder + 'data.xlsx'\n",
        "models_folder = data_folder + 'models/'\n",
        "\n",
        "# Cr√©er les dossiers s'ils n'existent pas\n",
        "os.makedirs(data_folder, exist_ok=True)\n",
        "os.makedirs(csv_folder, exist_ok=True)\n",
        "os.makedirs(models_folder, exist_ok=True)\n",
        "\n",
        "path = Path(data_xlsx_file)\n",
        "if path.is_file():\n",
        "    print(\"üëç Found data file at location: \" + data_xlsx_file)\n",
        "else:\n",
        "    csv_file = csv_folder + 'aiSMB_Newrecords.csv'\n",
        "    if not os.path.exists(csv_file):\n",
        "        raise Exception(\"CSV file not found in location: \" + csv_file)\n",
        "    else:\n",
        "        print(\"CSV file found at location: \" + csv_file)\n",
        "\n"
      ],
      "metadata": {
        "id": "oneIsBk3zFNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#second step : creating the file data.xlsx\n",
        "import pandas as pd\n",
        "import openpyxl\n",
        "import datetime\n",
        "import warnings\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
        "\n",
        "\n",
        "\n",
        "# Lire le fichier CSV\n",
        "csv_file = '/content/drive/My Drive/nightscout-ml/csvfile/aiSMB_Newrecords.csv'\n",
        "data = pd.read_csv(csv_file,on_bad_lines=\"skip\")\n",
        "\n",
        "\n",
        "# Renommer les colonnes\n",
        "data.columns = ['dateStr', 'dateLong', 'hourOfDay', 'weekend',\n",
        "                'bg', 'targetBg', 'iob', 'cob', 'lastCarbAgeMin', 'futureCarbs', 'delta', 'shortAvgDelta', 'longAvgDelta',\n",
        "                'accelerating_up', 'deccelerating_up', 'accelerating_down', 'deccelerating_down', 'stable',\n",
        "                'tdd7DaysPerHour', 'tdd2DaysPerHour', 'tddDailyPerHour', 'tdd24HrsPerHour',\n",
        "                'recentSteps5Minutes', 'recentSteps10Minutes', 'recentSteps15Minutes', 'recentSteps30Minutes', 'recentSteps60Minutes',\n",
        "                'tags0to60minAgo', 'tags60to120minAgo', 'tags120to180minAgo', 'tags180to240minAgo',\n",
        "                'variableSensitivity', 'lastbolusage', 'predictedSMB', 'maxIob', 'maxSMB', 'smbToGive']\n",
        "\n",
        "# Effacer le contenu des colonnes mentionn√©es\n",
        "cols_to_remove = ['tags0to60minAgo', 'tags60to120minAgo', 'tags120to180minAgo', 'tags180to240minAgo']\n",
        "data[cols_to_remove] = None\n",
        "# Appliquer la condition\n",
        "data.loc[(data['predictedSMB'] == 0) & (data['smbToGive'] > 0), 'predictedSMB'] = data['smbToGive']\n",
        "\n",
        "# Chemin du fichier Excel\n",
        "excel_file = '/content/drive/My Drive/nightscout-ml/data.xlsx'\n",
        "\n",
        "# V√©rifier si le fichier existe et le renommer si n√©cessaire\n",
        "if os.path.exists(excel_file):\n",
        "    today = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
        "    new_file_name = f'/content/drive/My Drive/nightscout-ml/data_{today}.xlsx'\n",
        "    os.rename(excel_file, new_file_name)\n",
        "\n",
        "# Cr√©er un fichier Excel et ajouter les donn√©es √† la feuille \"training_data\"\n",
        "with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:\n",
        "    data.to_excel(writer, sheet_name='training_data', index=False)\n",
        "    \n",
        "    # Cr√©er un objet PieChart\n",
        "    chart = openpyxl.chart.PieChart()\n",
        "    \n",
        "    # Cr√©er une nouvelle feuille pour le temps dans la cible\n",
        "    workbook = writer.book\n",
        "    tir_sheet = workbook.create_sheet('TIR')\n",
        "\n",
        "    # Calculer le temps pass√© dans la cible\n",
        "    bg_range = data['bg'].tolist()\n",
        "    tir_count = len([x for x in bg_range if 65 <= x <= 180])\n",
        "    total_count = len(bg_range)\n",
        "    tir_percentage = round(tir_count / total_count * 100, 2)  # arrondir √† 2 d√©cimales\n",
        "\n",
        "    date_format_str = '%m/%d/%y %I:%M%p'\n",
        "\n",
        "    def time_to_str(date):\n",
        "        date_str = datetime.strftime(date, date_format_str)\n",
        "        \n",
        "        # drop leading 0 on month\n",
        "        date_str = date_str[1:] if date_str.startswith('0') else date_str\n",
        "        \n",
        "        date_str = date_str.replace('/0', '/')\n",
        "        return date_str\n",
        "    def convert_date(date_str):\n",
        "        try:\n",
        "            return datetime.datetime.strptime(date_str, '%m/%d/%y %H:%M').date()\n",
        "        except ValueError:\n",
        "            return None\n",
        "\n",
        "    data = pd.read_csv(csv_file, on_bad_lines=\"skip\")\n",
        "    data['dateStr'] = data['dateStr'].apply(convert_date)\n",
        "    data.dropna(subset=['dateStr'], inplace=True)\n",
        "\n",
        "    \n",
        "    # Obtenir la date actuelle\n",
        "    today = datetime.date.today()\n",
        "    \n",
        "    # R√©cup√©rer la premi√®re et la derni√®re date\n",
        "    #first_date = datetime.datetime.strptime(data['dateStr'].iloc[0], '%m/%d/%y %H:%M').date()\n",
        "    first_date = data['dateStr'].iloc[0]\n",
        "\n",
        "\n",
        "    # Calculer le nombre de jours entre les deux dates\n",
        "    nb_jours = (today - first_date).days\n",
        "    \n",
        "    # Ajouter le pourcentage et le nombre de jours √† la feuille \"TIR\"\n",
        "    tir_sheet.cell(row=1, column=1, value=\"TIR\")\n",
        "    tir_sheet.cell(row=1, column=2, value=\"%\")\n",
        "    tir_sheet.cell(row=1, column=3, value=\"NB DAYS\")\n",
        "    tir_sheet.cell(row=2, column=1, value=\"65-180\")\n",
        "    tir_sheet.cell(row=2, column=2, value=tir_percentage)\n",
        "    tir_sheet.cell(row=2, column=3, value=nb_jours)\n",
        "    # Calculer le temps pass√© dans la cible\n",
        "    bg_range = data['bg'].tolist()\n",
        "    tir_count = len([x for x in bg_range if 65 <= x <= 180])\n",
        "    total_count = len(bg_range)\n",
        "    tir_percentage = [round(tir_count / total_count * 100, 2), 100 - round(tir_count / total_count * 100, 2)]\n",
        "\n",
        "    # D√©finir les √©tiquettes\n",
        "    labels = ['Temps pass√© dans la cible (65-180)', 'Temps pass√© hors de la cible']\n",
        "\n",
        "    # Cr√©er un diagramme √† barres avec des couleurs personnalis√©es\n",
        "    colors = ['#008000', '#FFFF00']\n",
        "    x = np.arange(len(labels))  # les positions des √©tiquettes sur l'axe des x\n",
        "    width = 0.35  # la largeur des barres\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    barres = ax.bar(x, tir_percentage, width, color=colors)\n",
        "\n",
        "    # Ajouter des √©tiquettes, un titre et des axes personnalis√©s\n",
        "    ax.set_ylabel('TIR %')\n",
        "    ax.set_title(f\"TIR (65-180) period : {nb_jours} DAYS\")\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(labels)\n",
        "\n",
        "    # Fonction pour ajouter les pourcentages au-dessus des barres\n",
        "    def autolabel(rects):\n",
        "        for rect in rects:\n",
        "            height = rect.get_height()\n",
        "            ax.annotate('{:.1f}%'.format(height),\n",
        "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                        xytext=(0, 3),\n",
        "                        textcoords=\"offset points\",\n",
        "                        ha='center', va='bottom')\n",
        "\n",
        "    autolabel(barres)\n",
        "\n",
        "    # Enregistrer le graphique comme image\n",
        "    plt.savefig(\"/content/drive/My Drive/nightscout-ml/graphique.png\")\n",
        "    plt.clf()\n",
        "\n",
        "    # Afficher le diagramme √† barres\n",
        "    plt.show()\n",
        "\n",
        "    # Ajouter le graphique √† la feuille \"TIR\"\n",
        "    from openpyxl.drawing.image import Image\n",
        "    \n",
        "    cwd = os.getcwd()\n",
        "    chart_file = os.path.join(cwd, \"/content/drive/My Drive/nightscout-ml/graphique.png\")\n",
        "    img = Image(chart_file)\n",
        "    tir_sheet.add_image(img, \"D2\")\n",
        "    \n",
        "    # Sauvegarder le fichier Excel\n",
        "    writer.save()"
      ],
      "metadata": {
        "id": "Acz-zmNF2EQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step 3 : validating the date format\n",
        "from datetime import datetime\n",
        "\n",
        "date_format_str = '%m/%d/%y %I:%M%p'\n",
        "\n",
        "def time_to_str(date):\n",
        "    date_str = datetime.strftime(date, date_format_str)\n",
        "    \n",
        "    # drop leading 0 on month\n",
        "    date_str = date_str[1:] if date_str.startswith('0') else date_str\n",
        "    \n",
        "    date_str = date_str.replace('/0', '/')\n",
        "    return date_str\n",
        "\n",
        "def str_to_time(date_str):\n",
        "    return datetime.strptime(date_str, date_format_str)\n"
      ],
      "metadata": {
        "id": "z5Et-tfKKNCZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step 4 : creating the model.tflite file\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from datetime import datetime\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import PReLU\n",
        "import time\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "pd.set_option('display.width', 3000)\n",
        "now = datetime.now()\n",
        "date_str = \"{}-{}-{}_{}-{}\".format(now.year, now.month, now.day, now.hour, now.minute)\n",
        "time_ranges = ['0to60minAgo', '60to120minAgo', '120to180minAgo', '180to240minAgo']\n",
        "log_folder = 'logs'\n",
        "\n",
        "ISF = [\"variableSensitivity\"]\n",
        "time_bolus = [\"lastbolusage\"]\n",
        "hour_breakdowns = [\"hour0_2\",\"hour3_5\",\"hour6_8\",\"hour9_11\",\"hour12_14\",\"hour15_17\",\"hour18_20\",\"hour21_23\"]\n",
        "accellerating = [\"accelerating_up\",\"deccelerating_up\",\"accelerating_down\",\"deccelerating_down\",\"stable\"]\n",
        "recent_steps = [\"recentSteps5Minutes\",\"recentSteps10Minutes\",\"recentSteps15Minutes\",\"recentSteps30Minutes\",\"recentSteps60Minutes\"]\n",
        "sleep_seated = [\"sleep\",\"sedentary\"]\n",
        "tdd = [\"tdd7Days\",\"tddDaily\",\"tdd24Hrs\"]\n",
        "tddPerHour = [\"tdd7DaysPerHour\",\"tdd2DaysPerHour\",\"tddDailyPerHour\",\"tdd24HrsPerHour\"]\n",
        "base_cols = [\n",
        "                \"hourOfDay\",\"weekend\",\"bg\",\n",
        "             #\"targetBg\",\n",
        "              \"iob\",\n",
        "             #\"cob\",\"lastCarbAgeMin\",\"futureCarbs\",\n",
        "                #\"bg\",\"targetBg\",\"iob\",\n",
        "                \"delta\",\"shortAvgDelta\",\"longAvgDelta\"]\n",
        "cob_delta = ['cobDelta']\n",
        "gi_tags = ['giFast', 'giMedium', 'giSlow']\n",
        "prior_bgs_to_add = 36\n",
        "\n",
        "col_map = {\n",
        "    'best_cols': base_cols+tddPerHour,\n",
        "    #'best_cols': base_cols+tddPerHour+recent_steps,\n",
        "    # 'best_cols_cob_delta': base_cols+tddPerHour+cob_delta+recent_steps+cob_delta,\n",
        "    # 'best_cols_plus_glycemic_index': base_cols+tddPerHour+recent_steps+gi_tags,\n",
        "    # 'best_cols_plus_fast_med_slow': base_cols+tddPerHour+recent_steps+['fastCarbs','mediumCarbs','slowCarbs']\n",
        "    # 'best_cols_plus_prior_bgs_and_smbs': base_cols+tddPerHour\n",
        "    # 'best_cols_w_accell': base_cols+tddPerHour+cob_delta+recent_steps+accellerating,\n",
        "    \n",
        "    ### meal tags didn't improve model accuracy sadly :(\n",
        "    # 'best_cols_plus_tags': base_cols+tddPerHour+cob_delta+recent_steps\n",
        "\n",
        "    #'base': base_cols, # comment this line\n",
        "    # 'base_cobDelta': base_cols+cob_delta, # helps a little\n",
        "    # 'base_tddPerhour': base_cols+tddPerHour, # helpful\n",
        "    # 'base_recentSteps': base_cols+recent_steps, # helpful\n",
        "    # 'base_accellerating': base_cols+accellerating, # helpful, but could probably refine\n",
        "    \n",
        "    # 'base_tdd': base_cols+tdd, # helps, but tddPerHour values are more effective\n",
        "    # 'base_tdd_tddPerHour': base_cols+tdd+tddPerHour, # not more helpful than tddPerHour\n",
        "    # 'base_recentSteps_sleepSeated': base_cols+recent_steps+sleep_seated, # no major diff, steps is fine\n",
        "    # 'base_sleepSeated': base_cols+sleep_seated, # no major diff thus drop\n",
        "    # 'base_hour_breakdowns': base_cols+hour_breakdowns # not helpful thus drop\n",
        "}\n",
        "\n",
        "# Define model hyper parameters such as input values and nural network dimensions\n",
        "HP_COLS = hp.HParam('cols', hp.Discrete(list(col_map.keys())))\n",
        "\n",
        "HP_NUM_NODES_L1 = hp.HParam('num_nodes_l1', hp.Discrete([15])) \n",
        "HP_NUM_NODES_L2 = hp.HParam('num_nodes_l2', hp.Discrete([10]))\n",
        "HP_NUM_NODES_L3 = hp.HParam('num_nodes_l3', hp.Discrete([5]))\n",
        "HP_NUM_NODES_L4 = hp.HParam('num_nodes_l4', hp.Discrete([0]))\n",
        "HP_NUM_EPOCHS = hp.HParam('num_epochs', hp.Discrete([5]))\n",
        "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([.01]))\n",
        "HP_LAST_ACTIVATION = hp.HParam('last_activation', hp.Discrete(['prelu']))\n",
        "METRIC_LOSS = 'loss'\n",
        "\n",
        "def build_model(run_dir, hparams, train_features, train_labels, test_features, test_labels):\n",
        "    loss = 1\n",
        "    model = ''\n",
        "    with tf.summary.create_file_writer(run_dir).as_default():\n",
        "        hp.hparams(hparams)  # record the values used in this trial\n",
        "        model, loss = train_test_model(hparams, train_features, train_labels, test_features, test_labels)\n",
        "        tf.summary.scalar(METRIC_LOSS, loss, step=1)\n",
        "    return model, loss\n",
        "\n",
        "def train_test_model(hparams, train_features, train_labels, test_features, test_labels):\n",
        "    normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "    normalizer.adapt(np.array(train_features))\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Input(shape=(train_features.shape[1],)))\n",
        "    model.add(normalizer)\n",
        "    if hparams[HP_NUM_NODES_L1] > 0:\n",
        "        model.add(layers.Dense(units=hparams[HP_NUM_NODES_L1], activation=\"relu\"))\n",
        "    if hparams[HP_NUM_NODES_L2] > 0:\n",
        "        model.add(layers.Dense(units=hparams[HP_NUM_NODES_L2], activation=\"relu\"))\n",
        "    if hparams[HP_NUM_NODES_L3] > 0 and hparams[HP_NUM_NODES_L2] > 0:\n",
        "        model.add(layers.Dense(units=hparams[HP_NUM_NODES_L3], activation=\"relu\"))\n",
        "    if hparams[HP_NUM_NODES_L4] > 0 and hparams[HP_NUM_NODES_L3] > 0 and hparams[HP_NUM_NODES_L2] > 0:\n",
        "        model.add(layers.Dense(units=hparams[HP_NUM_NODES_L4], activation=\"relu\"))\n",
        "\n",
        "    if hparams[HP_LAST_ACTIVATION] == 'prelu':\n",
        "        prelu = PReLU()\n",
        "        model.add(layers.Dense(units=1, activation=prelu))\n",
        "    else:\n",
        "        model.add(layers.Dense(units=1, activation=hparams[HP_LAST_ACTIVATION]))\n",
        "\n",
        "    model.compile(\n",
        "        # optimizer=tf.optimizers.Adam(learning_rate=hparams[HP_LEARNING_RATE]),\n",
        "        optimizer='adam',\n",
        "        loss='mean_absolute_error')\n",
        "\n",
        "    model.fit(\n",
        "        train_features,\n",
        "        train_labels,\n",
        "        epochs=hparams[HP_NUM_EPOCHS],\n",
        "        # Suppress logging.\n",
        "        verbose=1,\n",
        "        # Calculate validation results on 20% of the training data.\n",
        "        validation_split = 0.2)\n",
        "    \n",
        "    loss = model.evaluate(test_features, test_labels)\n",
        "    print(train_features.head(10))\n",
        "    print(train_labels.head(10))\n",
        "    print(test_features.head(10))\n",
        "    print(test_labels.head(10))\n",
        "    print(f\"Trained model with {len(model.layers)} layers and loss of {loss}\")\n",
        "    return model, loss\n",
        "\n",
        "def add_glycemic_index_cols(df):\n",
        "    # load vocabulary map from csv\n",
        "    df_gi = pd.read_csv('glycemic_index.csv')\n",
        "    for index, row in df_gi.iterrows():\n",
        "        tag = row['tag']\n",
        "        gi = row['index']\n",
        "        \n",
        "        # if 0-60 min contains fast acting carb word, then set fast GI to 1\n",
        "        if gi == 'fast':\n",
        "            df['giFast'] = df['tags0to60minAgo'].str.contains(tag)\n",
        "            df['giFast'] = df['giFast'].astype(int)\n",
        "        \n",
        "        # if 0-60 or 60-120 contains medium acting carb word, then set medium GI col to 1\n",
        "        if gi == 'medium':\n",
        "            df['giMedium'] = df['tags0to60minAgo'].str.contains(tag)\n",
        "            df['giMedium'] = df['tags60to120minAgo'].str.contains(tag)\n",
        "            df['giMedium'] = df['giMedium'].astype(int)\n",
        "        \n",
        "        # if tags contain slow acting word, set slow GI column to 1\n",
        "        if gi == 'slow':\n",
        "            df['giSlow'] = df['tags60to120minAgo'].str.contains(tag)\n",
        "            df['giSlow'] = df['tags120to180minAgo'].str.contains(tag)\n",
        "            df['giSlow'] = df['tags180to240minAgo'].str.contains(tag)\n",
        "            df['giSlow'] = df['giSlow'].astype(int)\n",
        "        \n",
        "    return df\n",
        "\n",
        "def convert_tags_to_cols(df):\n",
        "    vocabulary = get_vocabulary_from_tags(df)\n",
        "    \n",
        "    # fill in NaN values with empty strings to support setting tags\n",
        "    for range in time_ranges:\n",
        "        df['tags'+range] = df['tags'+range].fillna('')\n",
        "\n",
        "    for word in vocabulary:\n",
        "        for range in time_ranges:\n",
        "            col_name = word+'_'+range\n",
        "            df[col_name] = df['tags'+range].str.contains(word)\n",
        "            df[col_name] = df[col_name].astype(int)\n",
        "    return df, vocabulary\n",
        "\n",
        "def get_vocabulary_from_tags(df):\n",
        "    # get list of unique words\n",
        "    words = list(df['tags0to60minAgo'].str.split(' ', expand=True).stack().unique())\n",
        "    stop_words = ['for', 'a', 'an', 'and', 'of', 'with', '']\n",
        "    \n",
        "    # vocaublary = [word.lower() for word in vocaublary]\n",
        "    vocabulary = []\n",
        "    for word in words:\n",
        "        word = word.lower()\n",
        "        word = word[:-2] if word.endswith(\"es\") else word\n",
        "        word = word[:-1] if word.endswith(\"s\") else word\n",
        "        if word not in stop_words and word not in vocabulary:\n",
        "            vocabulary.append(word)\n",
        "    # strip commas, strip stop words (and, a) - android strips a/an/and\n",
        "    # stem words if need be\n",
        "    return vocabulary\n",
        "\n",
        "def get_basic_model_desc(model):\n",
        "    model_info = \"\\n\\n------------\\n\"\n",
        "    model_info += f\"Model {date_str}\\n\"\n",
        "    model_info += f\"{len(model.layers)} Layers:\\n\"\n",
        "    for i in range (len(model.layers)):\n",
        "        layer = model.layers[i]\n",
        "        layer_info = f\"    - Layer {i}  - {layer.name}\"\n",
        "        layer_info += f\" ({layer.units})\" if 'dense' in layer.name else \"\"\n",
        "        layer_info += f\" ({layer.rate})\" if 'dropout' in layer.name else \"\"\n",
        "        model_info += layer_info + \"\\n\"\n",
        "    return model_info\n",
        "\n",
        "def save_model_info_v2(model, cols, best_loss, num_epochs, data_row_count, training_time, best_last_activation, best_learning_rate, vocabulary):\n",
        "    model_info = get_basic_model_desc(model)\n",
        "\n",
        "    model_info += f\"Model Loss & Accuracy: {str(round(best_loss, 5))}\\n\"\n",
        "    model_info += f\"Number of Epochs: {num_epochs} \\n\"\n",
        "    model_info += f\"Columns ({len(cols)}): {cols} - {col_map[cols]} \\n\"\n",
        "    model_info += f\"Vocabulary ({len(vocabulary)}) {vocabulary}\\n\"\n",
        "    model_info += f\"Training Data Size: {data_row_count} \\n\"\n",
        "    model_info += f\"Learning Rate: {best_learning_rate} \\n\"\n",
        "    model_info += f\"Activation: {best_last_activation} \\n\" if best_last_activation is not None else \"Activation: None\\n\"\n",
        "    # model_info += basic_predictions(model) + \"\\n\"\n",
        "    model_info += f\"Took {time.strftime('%H:%M:%S', time.gmtime(training_time))} to train\\n\"\n",
        "    model_info += \"NOTES: \\n\"\n",
        "    open(data_folder+'/models/tf_model_results.txt', \"a\").write(model_info)\n",
        "\n",
        "\n",
        "\n",
        "def basic_predictions(model):\n",
        "    if len(current_cols) != 38:\n",
        "        return f\"ERROR: incorrect number of columns ({len(current_cols)})\"\n",
        "\n",
        "    low = basic_predict(model,50,0.0,0.0,0)\n",
        "    low_w_iob = basic_predict(model,50,1.0,0.0,0)\n",
        "    normal_w_iob = basic_predict(model,100,1.0,0.0,0)\n",
        "    normal_wo_iob = basic_predict(model,100,0.0,0.0,0)\n",
        "    high_bg = basic_predict(model,200,0.0,0.0,0)\n",
        "    high_cob = basic_predict(model,100,1.0,30.0,0)\n",
        "    high_both = basic_predict(model,200,1.0,30.0,0)\n",
        "    line =  f\"    low: {low}    low_w_iob: {low_w_iob}    normal_w_iob: {normal_w_iob}    normal_wo_iob: {normal_wo_iob}\\n\"\n",
        "    line += f\"    high_bg: {high_bg}    high_cob: {high_cob}    high_both: {high_both}    high_both_and_rising {basic_predict(model, 200, 1, 60, 10)}\\n\"\n",
        "    line += f\"    low_rising  : {basic_predict(model, 70, 0, 20, 10)}    normal_rising  : {basic_predict(model, 100, 0, 20, 10)}    high_rising  : {basic_predict(model, 180, 0, 20, 10)}\\n\"\n",
        "    line += f\"    low_dropping: {basic_predict(model, 70, 2, 0, -7)}    normal_dropping: {basic_predict(model, 100, 2, 0, -7)}    high_dropping: {basic_predict(model, 180, 2, 0, -7)}\\n\"\n",
        "    return line\n",
        "    \n",
        "def basic_predict(model, bg, iob, cob, delta):\n",
        "    last_cob_min = 0 if cob == 0 else 5\n",
        "    accelerating_up = 1 if delta > 3 else 0\n",
        "    deccelerating_down = 1 if delta < -3 else 0\n",
        "    stable= 1 if delta > -3 and delta < 3 else 0\n",
        "    prediction = model.predict([[11,1,0,0, 0,0,0, 0,0,0, \n",
        "                    bg,100,iob,cob,last_cob_min,0,delta,delta,delta,\n",
        "                    accelerating_up, 0, deccelerating_down, 0, stable, \n",
        "                    33,1, 33,1, 33,1,\n",
        "                    0,0,0, 0,0,\n",
        "                    0,1]])\n",
        "                    # \"hourOfDay\",\"hour0_2\",\"hour3_5\",\"hour6_8\", \"hour9_11\",\"hour12_14\",\"hour15_17\", \"hour18_20\",\"hour21_23\",\"weekend\",\n",
        "                    # \"bg\",\"targetBg\",\"iob\",\"cob\",\"lastCarbAgeMin\",\"futureCarbs\",\"delta\",\"shortAvgDelta\",\"longAvgDelta\",\n",
        "                    # \"accelerating_up\",\"deccelerating_up\",\"accelerating_down\",\"deccelerating_down\",\"stable\",\n",
        "                    # \"tdd7Days\",\"tddDaily\",\"tddPerHour\",\"tdd24Hrs\",\n",
        "                    # \"tdd7Days\",\"tdd7DaysPerHour\", \"tddDaily\",\"tddDailyPerHour\", \"tdd24Hrs\",\"tdd24HrsPerHour\",\n",
        "                    # \"recentSteps5Minutes\",\"recentSteps10Minutes\",\"recentSteps15Minutes\", \"recentSteps30Minutes\",\"recentSteps60Minutes\",\n",
        "                    # \"sleep\",\"sedintary\",\n",
        "                    \n",
        "    return str(round(prediction[0][0], 3))\n",
        "\n",
        "def clear_log_folder():\n",
        "    if os.path.exists(log_folder):\n",
        "        shutil.rmtree(log_folder)\n",
        "    os.mkdir(log_folder)\n",
        "    if not os.path.exists(data_folder+'/models'):\n",
        "        os.mkdir(data_folder+'/models')\n",
        "\n",
        "\n",
        "\n",
        "def save_model(model):\n",
        "    # https://www.tensorflow.org/tutorials/keras/save_and_load#savedmodel_format\n",
        "    model.save(data_folder+'/models/backup/tf_model_'+date_str)\n",
        "\n",
        "    # https://medium.com/analytics-vidhya/running-ml-models-in-android-using-tensorflow-lite-e549209287f0\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model=model)\n",
        "    lite_model = converter.convert()\n",
        "    open(data_folder+'/models/backup/tf_model_'+date_str+'.tflite', \"wb\").write(lite_model)\n",
        "    open(data_folder+'/models/model.tflite', \"wb\").write(lite_model)\n",
        "\n",
        "\n",
        "def compare_two_models(model_1_date, model_2_date, row_dates):\n",
        "    m1 = tf.keras.models.load_model(f'{data_folder}/models/backup/tf_model_{model_1_date}')\n",
        "    m2 = tf.keras.models.load_model(f'{data_folder}/models/backup/tf_model_{model_2_date}')\n",
        "\n",
        "    df = pd.read_excel(data_xlsx_file,'training_data')\n",
        "    \n",
        "    # row_1 = row.copy()\n",
        "    # row_1.pop('tdd7DaysPerHour')\n",
        "    # row_1.pop('tdd24HrsPerHour')\n",
        "\n",
        "    eval_features = df.sample(frac=0.3, random_state=0)\n",
        "    eval_features = eval_features[current_cols]\n",
        "    eval_features_1 = eval_features.copy()\n",
        "    # eval_features_1.pop('tdd7DaysPerHour')\n",
        "    # eval_features_1.pop('tdd24HrsPerHour')\n",
        "    eval_features_1.pop('smbToGive')\n",
        "    eval_labels = eval_features.pop('smbToGive')\n",
        "\n",
        "    m1_eval = str(round(m1.evaluate(eval_features_1, eval_labels)[0], 6))\n",
        "    m2_eval = str(round(m2.evaluate(eval_features, eval_labels)[0], 6))\n",
        "    eval_diff = str(round((float(m2_eval) - float(m1_eval)), 5))\n",
        "\n",
        "    # m1_predict = str(round(m1.predict([row_1])[0][0],3))\n",
        "    # m2_predict = str(round(m2.predict([row])[0][0],3))\n",
        "\n",
        "    model_info = \" ---- Model Comparison ----\\n\"\n",
        "    model_info += f\" ---- MODELS: model 1: {model_1_date} - model 2: {model_2_date}\\n\"\n",
        "    model_info += f\" ---- EVAL LOSS:   model 1: {m1_eval} - model 2: {m2_eval} => \"\n",
        "    model_info += f\"{(eval_diff)} getting better \\n\\n\" if float(eval_diff) < 0 else f\"{(eval_diff)} !!! BAD !!! \\n\\n\"\n",
        "    for row_date in row_dates:\n",
        "        row = df.loc[df['dateStr'] == row_date]\n",
        "        row = row[current_cols]\n",
        "        smb_to_give = row['smbToGive'].values[0]\n",
        "        row.pop('smbToGive')\n",
        "        m1_predict = str(round(m1.predict([row])[0][0],3))\n",
        "        m2_predict = str(round(m2.predict([row])[0][0],3))\n",
        "        model_info += f\" ------ PREDICTIONS: {row_date} \"\n",
        "        model_info += f\"({smb_to_give}u, {row['bg'].values[0]}mg/dL, delta: {row['delta'].values[0]}, shortAvgDelta: {row['shortAvgDelta'].values[0]})\"\n",
        "        model_info += f\" - model 1: {m1_predict} - model 2: {m2_predict}\\n\"\n",
        "    \n",
        "    open(data_folder+'/models/tf_model_results.txt', \"a\").write(model_info)\n",
        "\n",
        "\n",
        "\n",
        "def build_tf_regression():\n",
        "    start = time.time()\n",
        "    df = pd.read_excel(data_xlsx_file,'training_data')\n",
        "    # df = df[100:9566]\n",
        "    clear_log_folder()\n",
        "\n",
        "    with tf.summary.create_file_writer(f'{log_folder}/hparam_tuning').as_default():\n",
        "        hp.hparams_config(\n",
        "            hparams=[HP_COLS, HP_NUM_NODES_L1, HP_NUM_NODES_L2, HP_NUM_NODES_L3, HP_NUM_NODES_L4, \n",
        "            HP_NUM_EPOCHS, HP_LEARNING_RATE, HP_LAST_ACTIVATION],\n",
        "            metrics=[hp.Metric(METRIC_LOSS, display_name='loss')],\n",
        "        )\n",
        "\n",
        "    if 'best_cols_plus_glycemic_index' in col_map.keys() or 'best_cols_plus_tags' in col_map.keys():\n",
        "        df, vocabulary = convert_tags_to_cols(df)\n",
        "        if 'best_cols_plus_glycemic_index' in col_map.keys():\n",
        "            df = add_glycemic_index_cols(df)\n",
        "    else:\n",
        "        vocabulary = ['not using tags']\n",
        "    train_dataset = df.sample(frac=0.8, random_state=0)\n",
        "    test_dataset = df.drop(train_dataset.index)\n",
        "\n",
        "    train_features = train_dataset.copy()\n",
        "    test_features = test_dataset.copy()\n",
        "\n",
        "    train_labels = train_features.pop('smbToGive')\n",
        "    test_labels = test_features.pop('smbToGive')\n",
        "\n",
        "    best_loss = 1\n",
        "    best_model = 1\n",
        "    best_epochs = 0\n",
        "    best_last_activation = ''\n",
        "    best_learning_rate = .1\n",
        "    best_cols = ''\n",
        "\n",
        "    \n",
        "    \n",
        "    session_num = 0\n",
        "\n",
        "    for cols_name in HP_COLS.domain.values:\n",
        "        columns = col_map[cols_name]\n",
        "        if 'plus_tags' in cols_name:\n",
        "            for time_range in time_ranges:\n",
        "                for word in vocabulary:\n",
        "                    columns.append(f\"{word}_{time_range}\")\n",
        "        if 'prior_bgs_and_smbs' in cols_name:\n",
        "            for offset in range(0, prior_bgs_to_add):\n",
        "                offset_in_5_min = (offset+1) * 5\n",
        "                columns.append(f\"{offset_in_5_min}_min_prior_bg\")\n",
        "                columns.append(f\"{offset_in_5_min}_min_prior_smb\")\n",
        "\n",
        "        iteration_train_features = train_features[columns]\n",
        "        iteration_test_features = test_features[columns]\n",
        "        for num_nodes_l1 in HP_NUM_NODES_L1.domain.values:\n",
        "            for num_nodes_l2 in HP_NUM_NODES_L2.domain.values:\n",
        "                for num_nodes_l3 in HP_NUM_NODES_L3.domain.values:\n",
        "                    for num_nodes_l4 in HP_NUM_NODES_L4.domain.values:\n",
        "                        for num_epochs in HP_NUM_EPOCHS.domain.values:\n",
        "                            for learn_rate in HP_LEARNING_RATE.domain.values:\n",
        "                                for last_activation in HP_LAST_ACTIVATION.domain.values:\n",
        "                                    hparams = {\n",
        "                                        HP_COLS: cols_name,\n",
        "                                        HP_NUM_NODES_L1: num_nodes_l1,\n",
        "                                        HP_NUM_NODES_L2: num_nodes_l2,\n",
        "                                        HP_NUM_NODES_L3: num_nodes_l3,\n",
        "                                        HP_NUM_NODES_L4: num_nodes_l4,\n",
        "                                        HP_NUM_EPOCHS: num_epochs,\n",
        "                                        HP_LEARNING_RATE: learn_rate,\n",
        "                                        HP_LAST_ACTIVATION: last_activation\n",
        "                                    }\n",
        "                                    run_name = f\"run-{cols_name}-{session_num}\"\n",
        "                                    print(f\"--- Starting trail {run_name}\")\n",
        "                                    print({h.name: hparams[h] for h in hparams})\n",
        "                                    model, loss = build_model('logs/hparam_tuning/' + run_name, hparams, iteration_train_features, train_labels, iteration_test_features, test_labels)\n",
        "                                    session_num += 1\n",
        "                                    if loss < best_loss:\n",
        "                                        best_loss = loss\n",
        "                                        best_model = model\n",
        "                                        best_epochs = num_epochs\n",
        "                                        best_learning_rate = learn_rate\n",
        "                                        best_last_activation = last_activation\n",
        "                                        best_cols = cols_name\n",
        "    \n",
        "    training_time = time.time() - start\n",
        "    \n",
        "    save_model_info_v2(best_model, best_cols, best_loss, best_epochs, len(df), training_time, best_last_activation, best_learning_rate, vocabulary)\n",
        "\n",
        "    save_model(best_model)\n",
        "    \n",
        "    return date_str\n",
        "\n",
        "build_tf_regression()"
      ],
      "metadata": {
        "id": "1kxMrMw0Kdwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5 : Testing Model\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import shutil\n",
        "import datetime\n",
        "source_folder = \"/content/drive/My Drive/nightscout-ml/models\"\n",
        "destination_folder = \"/content/drive/My Drive/nightscout-ml\"\n",
        "old_model_file_path = os.path.join(destination_folder, \"model-2.tflite\")\n",
        "new_model_file_path = os.path.join(source_folder, \"model.tflite\")\n",
        "\n",
        "# V√©rifier si l'ancien fichier model-2.tflite existe\n",
        "if os.path.isfile(old_model_file_path):\n",
        "    # Ajouter la date √† l'ancien fichier model-2.tflite\n",
        "    date_str = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
        "    renamed_old_model_file_path = os.path.join(destination_folder, f\"model-2_{date_str}.tflite\")\n",
        "    os.rename(old_model_file_path, renamed_old_model_file_path)\n",
        "    \n",
        "# Copier le fichier model.tflite vers le dossier de destination et le renommer en model-2.tflite\n",
        "shutil.copy(new_model_file_path, old_model_file_path)\n",
        "\n",
        "file_path = f'{data_folder}/model-2.tflite'\n",
        "if not Path(file_path).is_file():\n",
        "  raise Exception(\"Data file not found in location: \"+data_xlsx_file)\n",
        "\n",
        "# Load TFLite model and allocate tensors.\n",
        "interpreter = tf.lite.Interpreter(model_path=file_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "print(input_details)\n",
        "output_details = interpreter.get_output_details()\n",
        "print(output_details)\n",
        "\n",
        "def run_prediction(input_data):\n",
        "  # https://stackoverflow.com/questions/50443411/how-to-load-a-tflite-model-in-script\n",
        "  interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "  interpreter.invoke()\n",
        "  # The function `get_tensor()` returns a copy of the tensor data.\n",
        "  # Use `tensor()` in order to get a pointer to the tensor.\n",
        "  output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "  return output_data\n",
        "\n",
        "\n",
        "# Test model on random input data.\n",
        "input_shape = input_details[0]['shape']\n",
        "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
        "print(\"prediction with random data: \"+str(run_prediction(input_data)))\n",
        "\n",
        "# input columns: \n",
        "#   'hourOfDay', 'weekend', 'bg', 'targetBg', 'iob', 'cob', 'lastCarbAgeMin', 'futureCarbs', \n",
        "#   'delta', 'shortAvgDelta', 'longAvgDelta', \n",
        "#   'accelerating_up',\"'deccelerating_up','accelerating_down','deccelerating_down','stable'\n",
        "#   'variableSensitivity','lastbolusage'\n",
        "#   'tdd7DaysPerHour', 'tdd2DaysPerHour', 'tddDailyPerHour', 'tdd24HrsPerHour', \n",
        "#   'recentSteps5Minutes', 'recentSteps10Minutes', 'recentSteps15Minutes', 'recentSteps30Minutes', 'recentSteps60Minutes'\n",
        "input_data = [[\n",
        "              1, 0, 150, \n",
        "               #90, \n",
        "               1.5,\n",
        "               10, 10, 15,\n",
        "               #1,0, 0, 0, 0,\n",
        "               #1,\n",
        "               #100,\n",
        "               #100,\n",
        "               1.5, 1.5, 1.5, 1.5,\n",
        "               #300, 250, 0, 0, 0,\n",
        "               \n",
        "               ]]\n",
        "for i in range(len(input_data[0])):\n",
        "  # model requires float 32 numbers\n",
        "  input_data[0][i] = np.float32(input_data[0][i])\n",
        "prediction = run_prediction(input_data)\n",
        "print(\"prediction with input data: \"+str(run_prediction(input_data)))\n",
        "if 1 <= prediction <= 3:\n",
        "    print(\"Le mod√®le fonctionne car la pr√©diction est positive et proche ou sup√©rieure √† 1.\")\n",
        "else:\n",
        "    print(\"Le mod√®le ne fonctionne pas correctement.\")"
      ],
      "metadata": {
        "id": "j7BtzaJNYuvg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
